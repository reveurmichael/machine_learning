# Extensions v0.04: The Language Generation Phase (Heuristics Only)

## ðŸŽ¯ **Core Philosophy: Teaching an LLM to "Think" Like a Heuristic**

The `v0.04` extension is a unique and highly specialized version that applies **exclusively to the `heuristics` extension**. There is no `v0.04` for any other algorithm type.

Its sole purpose is to act as a **language-rich data producer** for the purpose of fine-tuning Large Language Models (LLMs). It answers the question:

> "Can we teach an LLM to replicate the logical, step-by-step reasoning of a classic heuristic algorithm?"

To do this, `heuristics-v0.04` modifies the heuristic agents from `v0.03` to not only calculate the next best move but also to **articulate in natural language *why* that move was chosen**.

## ðŸ—ï¸ **Architectural Approach: A Minor Evolution of v0.03**

The `v0.04` extension is not a major architectural leap. It is an enhancement of the existing `v0.03` structure.

*   **Foundation:** It inherits the full structure of `heuristics-v0.03`, including the `app.py`, `dashboard/`, and `scripts/` directories.
*   **Agent Modification:** The core change happens within the agent classes (e.g., `BFSAgent`, `AStarAgent`). These agents are modified to generate a human-readable "reasoning" string at each decision point.
*   **New Script:** A new script, `generate_jsonl_dataset.py`, is added to the `scripts/` directory. This script is dedicated to running the agents and saving their linguistic output in the required format.

```
extensions/heuristics-v0.04/
â”œâ”€â”€ ... (inherits all of v0.03's structure)
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ agent_bfs.py       # ðŸ‘ˆ MODIFIED: Now generates reasoning strings
â”‚   â””â”€â”€ agent_astar.py     # ðŸ‘ˆ MODIFIED: Now generates reasoning strings
â””â”€â”€ scripts/
    â”œâ”€â”€ ... (inherits scripts from v0.03)
    â””â”€â”€ generate_jsonl_dataset.py # ðŸ‘ˆ NEW: Dedicated script for this version's purpose
```

## ðŸ“œ **The Mandatory Data Format: JSONL for Fine-Tuning**

To create effective training data for LLMs, the output of `heuristics-v0.04` **must** be in the **JSONL (JSON Lines)** format. Each line in the output file represents a single training example, structured as a JSON object.

### **JSONL Schema**

Each JSON object must contain two keys: `"prompt"` and `"completion"`.

*   **`"prompt"`**: A textual description of the game state. This serves as the input to the LLM.
*   **`"completion"`**: A textual description of the agent's chosen move and the reasoning behind it. This is the target output the LLM will be trained to generate.

### **Example JSONL Output**

```json
{"prompt": "Current State: The snake is at [(5,5), (5,4)], the apple is at (8,8). The path is clear to the right.", "completion": "Move: RIGHT. Reasoning: The A* algorithm identifies a safe path to the right, which moves me closer to the apple while maintaining maximum distance from my own tail."}
{"prompt": "Current State: The snake is at [(2,2), (2,1)], the apple is at (2,5). The snake is trapped on the left and right.", "completion": "Move: UP. Reasoning: The BFS agent has determined that moving UP is the only escape route and a necessary step to reach the apple, even though it temporarily increases the distance."}
```

This format is specifically designed to be easily consumed by standard LLM fine-tuning pipelines (e.g., for Supervised Fine-Tuning).

## ðŸ“‹ **Compliance Checklist: The Definition of Done**

The `heuristics-v0.04` extension is considered complete and successful if:

- [ ] Does it build directly upon the `heuristics-v0.03` structure?
- [ ] Are the agent classes modified to generate natural language explanations for their moves?
- [ ] Is there a `scripts/generate_jsonl_dataset.py` script?
- [ ] Does this script produce a `.jsonl` file where each line is a valid JSON object?
- [ ] Does each JSON object strictly adhere to the `{"prompt": "...", "completion": "..."}` schema?

---

> **The `heuristics-v0.04` extension is a critical bridge between symbolic AI and neural AI. Its success is measured by its ability to produce clean, structured, and linguistically rich datasets for training the next generation of agents.**




















































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































> **Important â€” Authoritative Reference:** This document augments the _Final Decision Series_ (`final-decision-0` â†’ `final-decision-10`). If any statement here conflicts with the Final Decisions, the latter overrides.

## ðŸ§© General Overview

**v0.04** focuses exclusively on **dataset generation with natural language explanations**, specifically for the **heuristics** family of Snake game agents.

The core feature of v0.04 is the ability to generate **CSV datasets** that include both:

* **Structured decision data** (e.g., board state, direction, position),
* and **rich natural language descriptions** explaining **why each decision is made**.

This dataset is intended for **fine-tuning large language models (LLMs)** in Task 4, enabling them to learn and imitate heuristic algorithms through language-grounded reasoning.

---

## ðŸ¤– Heuristics v0.04

v0.04 applies **only to heuristics**.
There is **no v0.04** planned for:

* Supervised learning models
* Genetic or evolutionary algorithms
* reinforcement learning models
* any other extensions

### ðŸ” Built on Top of v0.03

Since v0.04 extends v0.03, it inherits the web interface, replay system, and dataset export infrastructure. However, v0.04 introduces a crucial innovation:

> At **each key decision point** in the heuristic logic, the agent will **generate detailed natural language explanations** describing the reasoning behind its action.

These explanations will be logged alongside standard metadata (e.g., grid state, next move) in CSV format.

### ðŸ§  LLM Fine-Tuning Use Case (Task 4)

These **language-rich CSV datasets** are designed specifically to:

* Teach LLMs how to play the Snake game
* Ground decision-making in explicit, interpretable reasoning
* Provide a training corpus for supervised fine-tuning (e.g., SFT or RFT) of open-weight models

Example data entries will include columns like:

| step | head\_pos | apple\_pos | move  | reasoning\_sentence                                                                    |
| ---- | --------- | ---------- | ----- | -------------------------------------------------------------------------------------- |
| 14   | (3,4)     | (6,7)      | RIGHT | "The apple is to the upper-right, and RIGHT leads me closer while avoiding obstacles." |

This forms the bridge from classic symbolic reasoning to neural policy learning.

---


## Maybe not CSV, but JSON JSONL ?

Great follow-up! For **fine-tuning large language models (LLMs)**, the dataset format and structure are quite different compared to traditional supervised learning models. Here's what you should consider:

---

### Dataset Format for Fine-Tuning LLMs

**1. Plain Text / JSONL (JSON Lines) â€” Most Common**

* **Format:** Usually, fine-tuning datasets for LLMs are stored in plain text or JSONL files.
* **Why?**

  * LLMs are trained on text tokens, so data needs to be text-based (natural language).
  * JSONL allows one JSON object per line, making it easy to store structured text pairs like prompt-completion, or input-output examples.
* **Example:**

```json
{"prompt": "Here is the current state of the game, head blabla, apple blabla, snake blabla, etc", "completion": "Move RIGHT by one cell to (x+1, y), because blabla (those texts are generated by the heuristics)"}
```

---

**2. Structured JSON or YAML**

* Sometimes more structured data is needed, like when including metadata, reasoning chains, or annotations.
* Useful if your heuristics produce **long language explanations or step-by-step reasoning** at each decision point.

---

**3. Text with Special Tokens**

* When fine-tuning, special tokens may be introduced to delineate prompts, instructions, or to mark start/end of completions.
* This is especially relevant for instruction-tuning or RLHF-style datasets.

---

### Why Not CSV/NPZ/Parquet for LLM Fine-Tuning?

* LLMs expect **sequences of tokens**, not tabular or numerical feature vectors.
* While technically possible to convert structured data to string format, CSV and binary formats are **not standard or efficient** for LLM fine-tuning pipelines.
* The dataset must be easily consumable by tokenizers and data loaders specific to language models.

---

### Dataset Structure & Content Considerations

Since you mentioned heuristics v0.04 will include **long language sentences explaining heuristic decisions**, your dataset might look like:

* **At each decision point (step):**

  * The current state description (in language)
  * The heuristic explanation (why a particular move was chosen)
  * The resulting action
  * Possibly a reward or outcome summary

This can be structured as a sequence of examples, each with prompt + completion text.

---

### Example Dataset Entry for Heuristic Explanation Fine-Tuning

```jsonl
{"prompt": "State: Snake head at (4,4), apple at (6,7). What move do you make?", 
 "completion": "I choose to move RIGHT because it shortens the distance to the apple while avoiding collisions."}
```

Or for longer explanations:

```jsonl
{"prompt": "Decision point 15:\nCurrent snake state and environment details...\nWhat is the reasoning and action?",
 "completion": "The heuristic evaluates that moving UP is safest because it leads to the open space, avoiding the snake's tail and obstacles. Therefore, the snake moves UP."}
```

## JSON VS JSONL

Great question! Here's a clear comparison between **JSON** and **JSONL (JSON Lines)** formats:

---

### JSON (JavaScript Object Notation)

* **Structure:** A single JSON document, often an array of objects.
* **Example:**

```json
[
  {"prompt": "Move right", "completion": "Snake moves right"},
  {"prompt": "Move up", "completion": "Snake moves up"}
]
```

* **Characteristics:**

  * The entire dataset is wrapped in a single JSON array or object.
  * Must be fully loaded and parsed at once by the program.
  * Can become very large and memory-intensive for big datasets.
  * Less suitable for streaming or incremental reading.

---

### JSONL (JSON Lines)

* **Structure:** Each line is a separate JSON object.
* **Example:**

```json
{"prompt": "Move right", "completion": "Snake moves right"}
{"prompt": "Move up", "completion": "Snake moves up"}
```

* **Characteristics:**

  * Easy to process line-by-line or stream.
  * More scalable for very large datasets.
  * Allows appending new data easily by adding new lines.
  * Supported by many ML frameworks (e.g., Hugging Face datasets expect JSONL).
  * Simpler to handle in shell commands and Unix pipelines.

---

### Which to Choose for LLM Fine-Tuning?

* **JSONL is preferred** because:

  * Allows incremental loading without memory overhead.
  * Fits better with data loading pipelines for tokenizers and training frameworks.
  * Easier to append, update, or split datasets.
  * Standard format in the LLM fine-tuning community.
  
---

### Summary for LLM Fine-Tuning Dataset Format:

| Format          | Typical Usage                   | Notes                                       |
| --------------- | ------------------------------- | ------------------------------------------- |
| JSONL (text)    | Prompt-completion pairs         | Standard for LLM fine-tuning                |
| Plain Text      | Raw or formatted text sequences | Simple but less structured                  |
| JSON/YAML       | Complex data with annotations   | Useful for metadata or multi-field examples |
| CSV/NPZ/Parquet | Not typical / Not recommended   | Not aligned with token-based input          |

---

### Final Advice:

* Design your dataset generation for heuristics v0.04 to **output JSONL files** with rich natural language explanations.
* Include clear prompt and completion fields for each decision step.
* Consider tokenization and model input length constraints (split or chunk long explanations if needed).








## ðŸ“¦ Supervised Learning Models

There is **no v0.04 for supervised learning models**.

The existing supervised extensions (v0.01â€“v0.03) already:

* Train on datasets exported from heuristics v0.03
* Cover a wide variety of models (XGBoost, LightGBM, PyTorch NN/CNN/RNN/GNN)
* Include training, evaluation, and visualization components

These models do **not require** language-annotated data at this time.


---

## âœ… Summary

| Category           | v0.04 Support | Notes                                                               |
| ------------------ | ------------- | ------------------------------------------------------------------- |
| Heuristics         | âœ… Yes         | CSV + language reasoning for LLM fine-tuning                        |
| Supervised Models  | âŒ No          | v0.03 is sufficient for training + evaluation                       |
| Genetic Algorithms | âŒ No          | v0.03 remains the latest; not involved in language generation tasks |

**v0.04 = JSONL datasets for LLM fine-tuning with rich explanations.**

> **Important**: See `data-format-decision-guide.md` for complete format specifications. v0.04 uses JSONL exclusively for LLM fine-tuning purposes.



