{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3AHts_Z8ULNy",
        "outputId": "20516171-60a6-4c63-f7a1-5e8b0a222502"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10  Train: loss=1.0300, acc=0.6302  Val:   loss=0.4779, acc=0.8305\n",
            "Epoch 2/10  Train: loss=0.8603, acc=0.6948  Val:   loss=0.4396, acc=0.8458\n",
            "Epoch 3/10  Train: loss=0.8142, acc=0.7126  Val:   loss=0.3994, acc=0.8615\n",
            "Epoch 4/10  Train: loss=0.7776, acc=0.7254  Val:   loss=0.3789, acc=0.8711\n",
            "Epoch 5/10  Train: loss=0.7645, acc=0.7296  Val:   loss=0.3824, acc=0.8680\n",
            "Epoch 6/10  Train: loss=0.7406, acc=0.7390  Val:   loss=0.3711, acc=0.8750\n",
            "Epoch 7/10  Train: loss=0.7186, acc=0.7451  Val:   loss=0.3745, acc=0.8726\n",
            "Epoch 8/10  Train: loss=0.7071, acc=0.7495  Val:   loss=0.3608, acc=0.8773\n",
            "Epoch 9/10  Train: loss=0.6956, acc=0.7544  Val:   loss=0.3520, acc=0.8818\n",
            "Epoch 10/10  Train: loss=0.6837, acc=0.7592  Val:   loss=0.3466, acc=0.8839\n",
            "✅ Fine-tuning complete!\n"
          ]
        }
      ],
      "source": [
        "# ▶︎ 1. Imports & Device\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# ▶︎ 2. Data transforms & loaders\n",
        "# CIFAR-10 images are 32×32; VGG expects ≥224×224\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "\n",
        "train_transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize(256),\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std),\n",
        "    ]\n",
        ")\n",
        "\n",
        "val_transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std),\n",
        "    ]\n",
        ")\n",
        "\n",
        "train_ds = datasets.CIFAR10(\n",
        "    root=\"./data\", train=True, download=True, transform=train_transform\n",
        ")\n",
        "val_ds = datasets.CIFAR10(\n",
        "    root=\"./data\", train=False, download=True, transform=val_transform\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_ds, batch_size=64, shuffle=False, num_workers=4)\n",
        "\n",
        "# ▶︎ 3. Load & modify VGG16\n",
        "model = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1).to(device)\n",
        "\n",
        "# Freeze convolutional backbone\n",
        "for param in model.features.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Replace classifier head\n",
        "n_classes = 10\n",
        "model.classifier = nn.Sequential(\n",
        "    nn.Linear(25088, 4096),\n",
        "    nn.ReLU(inplace=True),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(4096, 1024),\n",
        "    nn.ReLU(inplace=True),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(1024, n_classes),\n",
        ").to(device)\n",
        "\n",
        "# ▶︎ 4. Loss & optimizer (only head params)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
        "\n",
        "\n",
        "# ▶︎ 5. Training / validation functions\n",
        "def train_epoch(model, loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for imgs, labels in loader:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(imgs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * imgs.size(0)\n",
        "        _, preds = outputs.max(1)\n",
        "        correct += preds.eq(labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "    return running_loss / total, correct / total\n",
        "\n",
        "\n",
        "def eval_epoch(model, loader, criterion):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in loader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            outputs = model(imgs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item() * imgs.size(0)\n",
        "            _, preds = outputs.max(1)\n",
        "            correct += preds.eq(labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    return running_loss / total, correct / total\n",
        "\n",
        "\n",
        "# ▶︎ 6. Full training loop\n",
        "n_epochs = 10\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion)\n",
        "    val_loss, val_acc = eval_epoch(model, val_loader, criterion)\n",
        "    print(\n",
        "        f\"Epoch {epoch}/{n_epochs}  \"\n",
        "        f\"Train: loss={train_loss:.4f}, acc={train_acc:.4f}  \"\n",
        "        f\"Val:   loss={val_loss:.4f}, acc={val_acc:.4f}\"\n",
        "    )\n",
        "\n",
        "print(\"✅ Fine-tuning complete!\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
