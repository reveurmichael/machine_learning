The following is a transcript of a course lecture, with a duration of 6 minutes. 

Hi guys. So we are officially in the second half of the semester of our Machine Learning course, and things are becoming more and more interesting, because we are now quite familiar with the most important concepts of machine learning. As a recap, what we have learned so far. We have learned about linear regression, in which we use linear combinations of features to predict targets. It's quite simple, basically high school level math plus a little bit of matrix operations, but it's very powerful. Then we got into logistic regression on week 3, in which we use a sigmoid function to model the probability of a binary outcome. So we see that by simply introducing a non-linear activation function, we can turn linear regression into a binary classifier. Then, we went into neural networks, which is basically logistic regression stacked on top of each other. And as the universal approximation theorem tells us, with just one hidden layer of sufficent number of neurons, we can approximate any continuous function. That's quite magical and so powerful, and we can leverage that to do a lot of cool stuffs, for digit recognition, for object detection, for image segmentation, and so on.

And most importantly, we have learned about the loss functions, gradient descent, and backpropagation for training models. And I also gave you the pain of writing things from scratch, for linear regression, logistic regression and neural networks, so that we can  get into those fundamentals really deeply. And we have manipulated with tensorfow playground, we have watched the videos of 3B1B, and everyone loves 3B1B, right. His videos are so good and so cool.

And now, we will be doing something equally cool, maybe even cooler. So it's a snake game, played by AI, or, if you want, played by neural networks, because nowadays if we talk about AI, we are almost always talking about neural networks. We can do it in a lot ways. And I will show you.

So the first way, which is very a la mode, is to use Large Language Models to play snake games. We just give the prompt to the LLM, and it will generate a series of actions to take, and the snake will move accordingly. Again, LLM is just neural network, but in very large scale. So there is nothing to fear about. OK, let's check it out. Here is the code, hosted on gitee.com, please just git stash and git pull and you will be able to see everything. And here is the streamlit app. First let's check DeepSeek, in replay mode. Wow, it's working, and with a lot of intelligence. I'm really impressed. Maybe we can play that with Mistral AI, in real time. Let's do that. Wow, it's very good, very fast and very smart, it's at an equal level with DeepSeek.


The second way, is to use Reinforcement Learning to play snake games. So for example, we can play that with DQN, which dates back to 2013, with this famous paper from DeepMind, "Playing Atari with Deep Reinforcement Learning", that I have sent you in the WeChat group. It's also there in our gitee.com repo. So let's check it out, with DQN. Yeah, it's working quite well. So here the things, Reinforcement learning, it's like let the agent learn to play the game by itself, by trial and error. It will explore, by taking random actions, and then we will give it some rewards, or penalties, for the actions it takes. And then it will update its policy, and try to do better. And let's see the code, the most important part is this one, the Neural Network. So the neural network is used to approximate the Q-value function and output the action with the highest Q-value. So the role of the neural network, again, is to approximate a very complex function, and that's about it. The input layer, is the game state, and the output layer, is the action. And the neural network is just for the feature extraction and the input-output mapping. Amazing, right?

And, of course, we can do that with simple neural networks, with just a neural network for classification. We know that before this AI booming, computers could already play snake games quite well, with heuristics algorithms, like A star algorithm. So we will be able to use the heuristics to generate a lot of training data, right. And then we can use that to train a simple neural network, for classification. Again, the input layer is the game state, and the output layer, is just an action, or more precisely, a direction, to move. So even without reinforcement learning, just a simple neural network for classification, like the one we used for digit recognition, we can do that. This is so cool. 

And we have other approaches to play snake games with AI. For example, here we have this paper from Stanford, https://web.stanford.edu/class/aa228/reports/2020/final16.pdf , Curriculum Learning with Snake . The idea is to use curriculum learning to train the neural network. So we will start with a very simple game settings, let's say a 4x4 board, and then we will gradually increase the complexity of the game settings, to things like 10x10 board, or even 20x20 board. So it's very similar to transfer learning. And we will do that in our practice session. 

We can also do that by fine-tuning those 7B or 14B LLM models, using heurstics generated data to teach the model to better play snake games. And then we can distill the knowledge into a smaller model, for real-time applications. But we will be doing that a bit later. 

Thank you guys. Have fun with AI and with Snake games. And if you want, you can see the Human Play Tab here. But donâ€™t do that during the class. Good, see you guys.



