# ğŸš€ å®Œæ•´ LoRA åˆå¹¶ä¸éƒ¨ç½²æµæ°´çº¿

ä¸€ä¸ªå…¨é¢çš„æŒ‡å—ï¼Œç”¨äºåˆå¹¶ LoRA é€‚é…å™¨ã€è½¬æ¢ä¸ºä¼˜åŒ–æ ¼å¼ï¼Œå¹¶ä½¿ç”¨å¤šç§æœåŠ¡é€‰é¡¹éƒ¨ç½²å¾®è°ƒçš„å¤§è¯­è¨€æ¨¡å‹ã€‚

## ğŸ“‹ ç›®å½•

1. [å‰ç½®æ¡ä»¶ä¸è®¾ç½®](#å‰ç½®æ¡ä»¶ä¸è®¾ç½®)
2. [LoRA åˆå¹¶ä¸å¸è½½](#lora-åˆå¹¶ä¸å¸è½½)
3. [æ¨¡å‹è½¬æ¢ä¸ä¼˜åŒ–](#æ¨¡å‹è½¬æ¢ä¸ä¼˜åŒ–)
4. [éƒ¨ç½²é€‰é¡¹](#éƒ¨ç½²é€‰é¡¹)
5. [æµ‹è¯•ä¸éªŒè¯](#æµ‹è¯•ä¸éªŒè¯)
6. [ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²](#ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²)
7. [é«˜çº§æŠ€æœ¯](#é«˜çº§æŠ€æœ¯)
8. [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)

## ğŸ›  å‰ç½®æ¡ä»¶ä¸è®¾ç½®

### ç³»ç»Ÿè¦æ±‚
- **GPU**: NVIDIA A100/H100ï¼ˆæ¨èï¼‰ã€RTX 4090+ æˆ– V100
- **å†…å­˜**: 32GB+ï¼ˆå¤§å‹æ¨¡å‹éœ€è¦ 64GB+ï¼‰
- **å­˜å‚¨**: 500GB+ NVMe SSD
- **CUDA**: 11.8+ æˆ– 12.1+

### ç¯å¢ƒè®¾ç½®
```bash
# å®‰è£…æ ¸å¿ƒä¾èµ–
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
pip install transformers peft accelerate bitsandbytes
pip install vllm ollama huggingface_hub safetensors

# ç”¨äºé‡åŒ–å’Œè½¬æ¢
pip install auto-gptq optimum[onnxruntime-gpu]
pip install llama-cpp-python --force-reinstall --no-cache-dir

# å…‹éš† llama.cpp ç”¨äº GGUF è½¬æ¢
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp && make -j $(nproc)
```

## ğŸ”— LoRA åˆå¹¶ä¸å¸è½½

### æ–¹æ³• 1: æ ‡å‡† PEFT åˆå¹¶
```python
#!/usr/bin/env python3
"""
é«˜çº§ LoRA åˆå¹¶è„šæœ¬ï¼Œæ”¯æŒå¤šç§åˆå¹¶ç­–ç•¥
"""
import torch
import gc
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel, PeftConfig
import argparse
from pathlib import Path
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def merge_lora_adapters(
    base_model_path: str,
    lora_adapter_path: str,
    output_path: str,
    merge_strategy: str = "linear",
    weights: list = None,
    dtype: torch.dtype = torch.bfloat16,
    device_map: str = "auto",
    max_memory: dict = None
):
    """
    ä½¿ç”¨é«˜çº§ç­–ç•¥å°† LoRA é€‚é…å™¨ä¸åŸºç¡€æ¨¡å‹åˆå¹¶ã€‚
    
    å‚æ•°:
        base_model_path: åŸºç¡€æ¨¡å‹è·¯å¾„
        lora_adapter_path: LoRA é€‚é…å™¨è·¯å¾„æˆ–è·¯å¾„åˆ—è¡¨
        output_path: åˆå¹¶æ¨¡å‹çš„è¾“å‡ºç›®å½•
        merge_strategy: åˆå¹¶å¤šä¸ªé€‚é…å™¨çš„ç­–ç•¥
        weights: åŠ æƒåˆå¹¶çš„æƒé‡
        dtype: å†…å­˜ä¼˜åŒ–çš„æ¨¡å‹æ•°æ®ç±»å‹
        device_map: è®¾å¤‡æ˜ å°„ç­–ç•¥
        max_memory: æ¯ä¸ªè®¾å¤‡çš„æœ€å¤§å†…å­˜
    """
    
    # ä½¿ç”¨ä¼˜åŒ–åŠ è½½åŸºç¡€æ¨¡å‹
    logger.info(f"ä» {base_model_path} åŠ è½½åŸºç¡€æ¨¡å‹")
    model = AutoModelForCausalLM.from_pretrained(
        base_model_path,
        torch_dtype=dtype,
        device_map=device_map,
        max_memory=max_memory,
        trust_remote_code=True,
        low_cpu_mem_usage=True,
        attn_implementation="flash_attention_2"  # é€‚ç”¨äºæ”¯æŒçš„æ¨¡å‹
    )
    
    # åŠ è½½åˆ†è¯å™¨
    tokenizer = AutoTokenizer.from_pretrained(base_model_path)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # å¤„ç†å¤šä¸ª LoRA é€‚é…å™¨
    if isinstance(lora_adapter_path, list):
        # åŠ è½½ç¬¬ä¸€ä¸ªé€‚é…å™¨
        model = PeftModel.from_pretrained(model, lora_adapter_path[0])
        
        # æ·»åŠ é¢å¤–çš„é€‚é…å™¨
        for i, adapter_path in enumerate(lora_adapter_path[1:], 1):
            model.load_adapter(adapter_path, adapter_name=f"adapter_{i}")
        
        # ä½¿ç”¨æŒ‡å®šç­–ç•¥åˆå¹¶å¤šä¸ªé€‚é…å™¨
        if merge_strategy in ["linear", "ties", "dare_linear", "magnitude_prune"]:
            adapter_names = [f"adapter_{i}" for i in range(len(lora_adapter_path))]
            if weights is None:
                weights = [1.0 / len(adapter_names)] * len(adapter_names)
            
            model.add_weighted_adapter(
                adapters=adapter_names,
                weights=weights,
                adapter_name="merged_adapter",
                combination_type=merge_strategy,
                density=0.7 if "prune" in merge_strategy else None
            )
            model.set_adapters("merged_adapter")
    else:
        # å•ä¸ªé€‚é…å™¨
        model = PeftModel.from_pretrained(model, lora_adapter_path)
    
    # åˆå¹¶å¹¶å¸è½½
    logger.info("å°† LoRA æƒé‡ä¸åŸºç¡€æ¨¡å‹åˆå¹¶...")
    merged_model = model.merge_and_unload()
    
    # æ¸…ç† GPU å†…å­˜
    del model
    torch.cuda.empty_cache()
    gc.collect()
    
    # ä¿å­˜åˆå¹¶åçš„æ¨¡å‹
    logger.info(f"ä¿å­˜åˆå¹¶åçš„æ¨¡å‹åˆ° {output_path}")
    Path(output_path).mkdir(parents=True, exist_ok=True)
    
    merged_model.save_pretrained(
        output_path,
        save_safetensors=True,
        max_shard_size="5GB"
    )
    tokenizer.save_pretrained(output_path)
    
    # ä¿å­˜æ¨¡å‹å¡ç‰‡
    model_card = f"""
---
library_name: transformers
license: apache-2.0
base_model: {base_model_path}
tags:
- fine-tuned
- lora-merged
inference: false
---

# åˆå¹¶åçš„æ¨¡å‹

æ­¤æ¨¡å‹æ˜¯é€šè¿‡å°† LoRA é€‚é…å™¨ä¸åŸºç¡€æ¨¡å‹åˆå¹¶è€Œåˆ›å»ºçš„ã€‚

## åŸºç¡€æ¨¡å‹
- {base_model_path}

## LoRA é€‚é…å™¨
- {lora_adapter_path}

## åˆå¹¶ç­–ç•¥
- {merge_strategy}

## ä½¿ç”¨æ–¹æ³•
```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("{output_path}")
tokenizer = AutoTokenizer.from_pretrained("{output_path}")
```
"""
    
    with open(Path(output_path) / "README.md", "w") as f:
        f.write(model_card)
    
    logger.info("âœ… LoRA åˆå¹¶æˆåŠŸå®Œæˆï¼")
    return output_path

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--base_model", required=True, help="åŸºç¡€æ¨¡å‹è·¯å¾„")
    parser.add_argument("--lora_adapter", required=True, help="LoRA é€‚é…å™¨è·¯å¾„")
    parser.add_argument("--output_path", required=True, help="è¾“å‡ºç›®å½•")
    parser.add_argument("--merge_strategy", default="linear", choices=["linear", "ties", "dare_linear", "magnitude_prune"])
    parser.add_argument("--dtype", default="bfloat16", choices=["float16", "bfloat16", "float32"])
    
    args = parser.parse_args()

    dtype_map = {
        "float16": torch.float16,
        "bfloat16": torch.bfloat16,
        "float32": torch.float32
    }
    
    merge_lora_adapters(
        base_model_path=args.base_model,
        lora_adapter_path=args.lora_adapter,
        output_path=args.output_path,
        merge_strategy=args.merge_strategy,
        dtype=dtype_map[args.dtype]
    )
```

### æ–¹æ³• 2: é«˜çº§å¤šé€‚é…å™¨åˆå¹¶
```python
def advanced_multi_adapter_merge(adapters_config: dict, output_path: str):
    """
    ä½¿ç”¨ä¸åŒç­–ç•¥çš„é«˜çº§åˆå¹¶ï¼Œé€‚ç”¨äºæ¯ç§é€‚é…å™¨ç±»å‹ã€‚
    
    adapters_config ç¤ºä¾‹:
    {
        "base_model": "path/to/base",
        "adapters": [
            {"path": "adapter1", "weight": 0.6, "type": "task"},
            {"path": "adapter2", "weight": 0.4, "type": "style"}
        ],
        "strategy": "ties",
        "density": 0.7
    }
    """
    # å¤æ‚åˆå¹¶åœºæ™¯çš„å®ç°
    pass
```

## âš¡ æ¨¡å‹è½¬æ¢ä¸ä¼˜åŒ–

### ä¸º llama.cpp è¿›è¡Œ GGUF è½¬æ¢
```python
#!/usr/bin/env python3
"""
é«˜çº§ GGUF è½¬æ¢ï¼Œæ”¯æŒå¤šç§é‡åŒ–é€‰é¡¹
"""
import subprocess
import os
from pathlib import Path
import json

def convert_to_gguf(
    model_path: str,
    output_dir: str,
    quantization: str = "Q4_K_M",
    vocab_type: str = "spm",  # spm, bpe, auto
    context_length: int = 32768,
    rope_freq_base: float = None,
    rope_freq_scale: float = None
):
    """
    å°† HuggingFace æ¨¡å‹è½¬æ¢ä¸º GGUF æ ¼å¼å¹¶è¿›è¡Œä¼˜åŒ–ã€‚
    """
    
    model_path = Path(model_path)
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # æ¨¡å‹ä¿¡æ¯
    config_path = model_path / "config.json"
    with open(config_path) as f:
        config = json.load(f)
    
    model_name = config.get("_name_or_path", model_path.name)
    
    # æ­¥éª¤ 1: è½¬æ¢ä¸º GGUF FP16
    fp16_path = output_dir / f"{model_name}-fp16.gguf"
    
    convert_cmd = [
        "python", "llama.cpp/convert-hf-to-gguf.py",
        str(model_path),
        "--outfile", str(fp16_path),
        "--outtype", "f16",
        "--vocab-type", vocab_type,
        "--pad-vocab"
    ]
    
    if context_length:
        convert_cmd.extend(["--ctx", str(context_length)])
    
    if rope_freq_base:
        convert_cmd.extend(["--rope-freq-base", str(rope_freq_base)])
    
    if rope_freq_scale:
        convert_cmd.extend(["--rope-freq-scale", str(rope_freq_scale)])
    
    print(f"ğŸ”„ è½¬æ¢ä¸º FP16 GGUF: {' '.join(convert_cmd)}")
    subprocess.run(convert_cmd, check=True)
    
    # æ­¥éª¤ 2: é‡åŒ–ä¸ºæŒ‡å®šæ ¼å¼
    if quantization != "f16":
        quantized_path = output_dir / f"{model_name}-{quantization}.gguf"
        
        quant_cmd = [
            "llama.cpp/quantize",
            str(fp16_path),
            str(quantized_path),
            quantization
        ]
        
        print(f"ğŸ”„ é‡åŒ–ä¸º {quantization}: {' '.join(quant_cmd)}")
        subprocess.run(quant_cmd, check=True)
        
        # åˆ é™¤ FP16 æ–‡ä»¶ä»¥èŠ‚çœç©ºé—´
        if fp16_path.exists():
            fp16_path.unlink()
        
        final_path = quantized_path
    else:
        final_path = fp16_path
    
    print(f"âœ… GGUF è½¬æ¢å®Œæˆ: {final_path}")
    return final_path

# é‡åŒ–é€‰é¡¹åŠè¯´æ˜
QUANTIZATION_OPTIONS = {
    "Q2_K": "æå°ï¼Œé«˜è´¨é‡æŸå¤±",
    "Q3_K_S": "å°ï¼Œæé«˜è´¨é‡æŸå¤±",
    "Q3_K_M": "ä¸­ç­‰ï¼Œé«˜è´¨é‡æŸå¤±",
    "Q3_K_L": "å¤§ï¼Œé«˜è´¨é‡æŸå¤±",
    "Q4_0": "ä¼ ç»Ÿï¼Œå°ï¼Œæé«˜è´¨é‡æŸå¤±",
    "Q4_1": "ä¼ ç»Ÿï¼Œå°ï¼Œå®è´¨æ€§è´¨é‡æŸå¤±",
    "Q4_K_S": "å°ï¼Œè¾ƒå¤§è´¨é‡æŸå¤±",
    "Q4_K_M": "ä¸­ç­‰ï¼Œå¹³è¡¡è´¨é‡/å¤§å°ï¼ˆæ¨èï¼‰",
    "Q5_0": "ä¼ ç»Ÿï¼Œä¸­ç­‰ï¼Œå¹³è¡¡è´¨é‡/å¤§å°",
    "Q5_1": "ä¼ ç»Ÿï¼Œä¸­ç­‰ï¼Œä½è´¨é‡æŸå¤±",
    "Q5_K_S": "å¤§ï¼Œä½è´¨é‡æŸå¤±",
    "Q5_K_M": "å¤§ï¼Œæä½è´¨é‡æŸå¤±ï¼ˆæ¨èï¼‰",
    "Q6_K": "æå¤§ï¼Œæä½è´¨é‡æŸå¤±",
    "Q8_0": "æå¤§ï¼Œæä½è´¨é‡æŸå¤±",
    "F16": "æå¤§ï¼Œæ— è´¨é‡æŸå¤±",
    "F32": "æœ€å¤§å¤§å°ï¼Œæ— è´¨é‡æŸå¤±"
}
```

### AWQ/GPTQ é‡åŒ–
```python
def quantize_with_awq(model_path: str, output_path: str, bits: int = 4):
    """
    ä½¿ç”¨ AutoAWQ é‡åŒ–æ¨¡å‹ä»¥è·å¾—æœ€å¤§æ¨ç†é€Ÿåº¦ã€‚
    """
    from awq import AutoAWQForCausalLM
    from transformers import AutoTokenizer
    
    # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
    model = AutoAWQForCausalLM.from_pretrained(
        model_path, 
        safetensors=True, 
        device_map="auto"
    )
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    
    # é‡åŒ–
    model.quantize(
        tokenizer,
        quant_config={
            "zero_point": True,
            "q_group_size": 128,
            "w_bit": bits,
            "version": "GEMM"
        }
    )
    
    # ä¿å­˜é‡åŒ–æ¨¡å‹
    model.save_quantized(output_path, safetensors=True)
    tokenizer.save_pretrained(output_path)
    
    print(f"âœ… AWQ é‡åŒ–å®Œæˆ: {output_path}")
```

## ğŸš€ éƒ¨ç½²é€‰é¡¹

### é€‰é¡¹ 1: Ollama éƒ¨ç½²
```bash
#!/bin/bash
# ollama_deploy.sh

MODEL_PATH="$1"
MODEL_NAME="$2"
QUANTIZATION="${3:-Q4_K_M}"

if [ -z "$MODEL_PATH" ] || [ -z "$MODEL_NAME" ]; then
    echo "ç”¨æ³•: $0 <model_path> <model_name> [quantization]"
    exit 1
fi

# åˆ›å»º Modelfile
cat > Modelfile << EOF
FROM ${MODEL_PATH}

# æ¸©åº¦æ§åˆ¶åˆ›é€ æ€§ (0.0-2.0)
PARAMETER temperature 0.7

# Top-p æ§åˆ¶å¤šæ ·æ€§ (0.0-1.0)
PARAMETER top_p 0.9

# Top-k é™åˆ¶ token é€‰æ‹© (1-100)
PARAMETER top_k 40

# é‡å¤æƒ©ç½šå‡å°‘é‡å¤ (1.0-1.5)
PARAMETER repeat_penalty 1.1

# ä¸Šä¸‹æ–‡çª—å£å¤§å°
PARAMETER num_ctx 32768

# åœæ­¢åºåˆ—
PARAMETER stop "<|im_end|>"
PARAMETER stop "<|endoftext|>"

# ç³»ç»Ÿæ¶ˆæ¯
SYSTEM """ä½ æ˜¯ä¸€ä¸ªåœ¨è´ªåƒè›‡æ¸¸æˆæ•°æ®ä¸Šè®­ç»ƒçš„ AI åŠ©æ‰‹ã€‚ä½ å¯ä»¥ä¸ºè´ªåƒè›‡æ¸¸æˆæä¾›ç­–ç•¥å»ºè®®ï¼Œåˆ†ææ¸¸æˆçŠ¶æ€ï¼Œå¹¶å»ºè®®æœ€ä½³ç§»åŠ¨ã€‚"""

# ç¤ºä¾‹å¯¹è¯
MESSAGE user "è´ªåƒè›‡çš„æœ€ä½³ç­–ç•¥æ˜¯ä»€ä¹ˆï¼Ÿ"
MESSAGE assistant "è´ªåƒè›‡çš„å…³é”®ç­–ç•¥åŒ…æ‹¬ï¼š1) æ²¿å¢™å£å’Œè¾¹ç¼˜ä¿æŒå®‰å…¨ï¼Œ2) åˆ›å»ºå¹¶ç»´æŠ¤å®‰å…¨è·¯å¾„ï¼Œ3) é¿å…åœ¨è§’è½ä¸­å›°ä½è‡ªå·±ï¼Œ4) æå‰è®¡åˆ’å‡ æ­¥ï¼Œ5) è€å¿ƒæ”¶é›†é£Ÿç‰©ã€‚"
EOF

# åœ¨ Ollama ä¸­åˆ›å»ºæ¨¡å‹
echo "ğŸ”„ åˆ›å»º Ollama æ¨¡å‹..."
ollama create $MODEL_NAME -f Modelfile

# æµ‹è¯•æ¨¡å‹
echo "ğŸ§ª æµ‹è¯•æ¨¡å‹..."
echo "ä»€ä¹ˆæ˜¯è´ªåƒè›‡æ¸¸æˆçš„æœ€ä½³ç­–ç•¥ï¼Ÿ" | ollama generate $MODEL_NAME

echo "âœ… Ollama éƒ¨ç½²å®Œæˆï¼"
echo "ç”¨æ³•: ollama run $MODEL_NAME"
```

### é€‰é¡¹ 2: vLLM éƒ¨ç½²
```python
#!/usr/bin/env python3
"""
ç”Ÿäº§ç¯å¢ƒ vLLM éƒ¨ç½²è„šæœ¬
"""
import argparse
from vllm import LLM, SamplingParams
from vllm.engine.arg_utils import AsyncEngineArgs
from vllm.engine.async_llm_engine import AsyncLLMEngine
import asyncio
import uvicorn
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Optional

class vLLMDeployment:
    def __init__(
        self,
        model_path: str,
        tensor_parallel_size: int = 1,
        pipeline_parallel_size: int = 1,
        max_model_len: int = 32768,
        gpu_memory_utilization: float = 0.85,
        quantization: Optional[str] = None,
        dtype: str = "bfloat16"
    ):
        self.engine_args = AsyncEngineArgs(
            model=model_path,
            tensor_parallel_size=tensor_parallel_size,
            pipeline_parallel_size=pipeline_parallel_size,
            max_model_len=max_model_len,
            gpu_memory_utilization=gpu_memory_utilization,
            quantization=quantization,
            dtype=dtype,
            enforce_eager=False,
            disable_log_stats=False,
            trust_remote_code=True
        )
        
        self.engine = AsyncLLMEngine.from_engine_args(self.engine_args)
        self.app = FastAPI(title="vLLM è´ªåƒè›‡æ¸¸æˆ API")
        self._setup_routes()
    
    def _setup_routes(self):
        @self.app.post("/v1/completions")
        async def generate_completion(request: CompletionRequest):
            try:
                sampling_params = SamplingParams(
                    temperature=request.temperature,
                    top_p=request.top_p,
                    top_k=request.top_k,
                    max_tokens=request.max_tokens,
                    stop=request.stop
                )
                
                results = await self.engine.generate(
                    request.prompt,
                    sampling_params,
                    request_id=f"req_{hash(request.prompt)}"
                )
                
                return {
                    "choices": [{
                        "text": output.text,
                        "finish_reason": output.finish_reason
                    } for output in results.outputs]
                }
                
            except Exception as e:
                raise HTTPException(status_code=500, detail=str(e))
        
        @self.app.get("/health")
        async def health_check():
            return {"status": "healthy"}
        
        @self.app.get("/v1/models")
        async def list_models():
            return {
                "data": [{
                    "id": "snake-game-model",
                    "object": "model",
                    "created": 1234567890,
                    "owned_by": "vllm"
                }]
            }
    
    def serve(self, host: str = "0.0.0.0", port: int = 8000):
        uvicorn.run(self.app, host=host, port=port)

class CompletionRequest(BaseModel):
    prompt: str
    temperature: float = 0.7
    top_p: float = 0.9
    top_k: int = 40
    max_tokens: int = 512
    stop: Optional[List[str]] = None

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--model", required=True, help="æ¨¡å‹è·¯å¾„")
    parser.add_argument("--host", default="0.0.0.0", help="ä¸»æœºåœ°å€")
    parser.add_argument("--port", type=int, default=8000, help="ç«¯å£å·")
    parser.add_argument("--tensor-parallel-size", type=int, default=1)
    parser.add_argument("--gpu-memory-utilization", type=float, default=0.85)
    
    args = parser.parse_args()

    deployment = vLLMDeployment(
        model_path=args.model,
        tensor_parallel_size=args.tensor_parallel_size,
        gpu_memory_utilization=args.gpu_memory_utilization
    )
    
    print(f"ğŸš€ å¯åŠ¨ vLLM æœåŠ¡å™¨åœ¨ {args.host}:{args.port}")
    deployment.serve(host=args.host, port=args.port)
```

### é€‰é¡¹ 3: TGI (Text Generation Inference)
```bash
#!/bin/bash
# tgi_deploy.sh

MODEL_PATH="$1"
PORT="${2:-8080}"

docker run --gpus all --shm-size 1g -p $PORT:80 \
    -v $MODEL_PATH:/data \
    ghcr.io/huggingface/text-generation-inference:latest \
    --model-id /data \
    --max-input-length 32768 \
    --max-total-tokens 65536 \
    --max-batch-prefill-tokens 8192 \
    --trust-remote-code
```

## ğŸ§ª æµ‹è¯•ä¸éªŒè¯

### ç»¼åˆæµ‹è¯•å¥—ä»¶
```python
#!/usr/bin/env python3
"""
ç»¼åˆæ¨¡å‹æµ‹è¯•å’ŒéªŒè¯å¥—ä»¶
"""
import requests
import time
import json
from typing import List, Dict
import numpy as np

class ModelTester:
    def __init__(self, api_url: str, model_name: str = None):
        self.api_url = api_url
        self.model_name = model_name
        
    def test_basic_functionality(self):
        """æµ‹è¯•åŸºæœ¬æ¨¡å‹åŠŸèƒ½"""
        test_prompts = [
            "å½“é£Ÿç‰©åœ¨æ­£å‰æ–¹æ—¶ï¼Œè´ªåƒè›‡çš„æœ€ä½³ç§»åŠ¨æ˜¯ä»€ä¹ˆï¼Ÿ",
            "å¦‚ä½•åœ¨è´ªåƒè›‡æ¸¸æˆä¸­é¿å…ç¢°æ’ï¼Ÿ",
            "æè¿°è´ªåƒè›‡æ¸¸æˆçš„æœ€ä½³ç­–ç•¥ã€‚",
            "å½“è›‡å˜é•¿æ—¶æˆ‘åº”è¯¥åšä»€ä¹ˆï¼Ÿ",
            "å¦‚ä½•åœ¨è´ªåƒè›‡ä¸­åˆ›å»ºå®‰å…¨è·¯å¾„ï¼Ÿ"
        ]
        
        results = []
        for prompt in test_prompts:
            start_time = time.time()
            response = self._generate(prompt)
            end_time = time.time()
            
            results.append({
                "prompt": prompt,
                "response": response,
                "latency": end_time - start_time,
                "tokens": len(response.split()) if response else 0
            })
        
        return results
    
    def benchmark_performance(self, num_requests: int = 10):
        """åŸºå‡†æµ‹è¯•æ¨¡å‹æ€§èƒ½"""
        prompt = "åˆ†æè¿™ä¸ªè´ªåƒè›‡æ¸¸æˆçŠ¶æ€å¹¶å»ºè®®æœ€ä½³ç§»åŠ¨ï¼š"
        
        latencies = []
        throughputs = []
        
        for i in range(num_requests):
            start_time = time.time()
            response = self._generate(prompt, max_tokens=100)
            end_time = time.time()
            
            latency = end_time - start_time
            tokens = len(response.split()) if response else 0
            throughput = tokens / latency if latency > 0 else 0
            
            latencies.append(latency)
            throughputs.append(throughput)
            
            print(f"è¯·æ±‚ {i+1}/{num_requests}: {latency:.2f}s, {throughput:.2f} tokens/s")
        
        return {
            "avg_latency": np.mean(latencies),
            "p50_latency": np.percentile(latencies, 50),
            "p95_latency": np.percentile(latencies, 95),
            "avg_throughput": np.mean(throughputs),
            "total_requests": num_requests
        }
    
    def _generate(self, prompt: str, max_tokens: int = 256) -> str:
        """ä»æ¨¡å‹ç”Ÿæˆå“åº”"""
        try:
            # æ ¹æ® API ç±»å‹é€‚é…
            if "ollama" in self.api_url:
                return self._ollama_generate(prompt)
            else:
                return self._openai_generate(prompt, max_tokens)
        except Exception as e:
            print(f"ç”Ÿæˆé”™è¯¯: {e}")
            return ""
    
    def _ollama_generate(self, prompt: str) -> str:
        """ä½¿ç”¨ Ollama API ç”Ÿæˆ"""
        response = requests.post(
            f"{self.api_url}/api/generate",
            json={"model": self.model_name, "prompt": prompt, "stream": False}
        )
        return response.json().get("response", "")
    
    def _openai_generate(self, prompt: str, max_tokens: int) -> str:
        """ä½¿ç”¨ OpenAI å…¼å®¹ API ç”Ÿæˆ"""
        response = requests.post(
            f"{self.api_url}/v1/completions",
            json={
                "prompt": prompt,
                "max_tokens": max_tokens,
                "temperature": 0.7,
                "stop": ["<|im_end|>"]
            }
        )
        return response.json()["choices"][0]["text"]

if __name__ == "__main__":
    # æµ‹è¯•ä¸åŒçš„éƒ¨ç½²
    deployments = [
        {"url": "http://localhost:11434", "name": "snake-model", "type": "ollama"},
        {"url": "http://localhost:8000", "name": "snake-model", "type": "vllm"},
    ]
    
    for deployment in deployments:
        print(f"\nğŸ§ª æµ‹è¯• {deployment['type']} éƒ¨ç½²...")
        tester = ModelTester(deployment["url"], deployment["name"])
        
        # åŸºæœ¬åŠŸèƒ½æµ‹è¯•
        results = tester.test_basic_functionality()
        print(f"âœ… åŸºæœ¬æµ‹è¯•å®Œæˆ: æµ‹è¯•äº† {len(results)} ä¸ªæç¤º")
        
        # æ€§èƒ½åŸºå‡†æµ‹è¯•
        benchmark = tester.benchmark_performance(10)
        print(f"ğŸ“Š æ€§èƒ½: {benchmark['avg_latency']:.2f}s å¹³å‡å»¶è¿Ÿ, {benchmark['avg_throughput']:.2f} tokens/s")
```

## ğŸ­ ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²

### Kubernetes éƒ¨ç½²
```yaml
# k8s-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: snake-model-deployment
spec:
  replicas: 2
  selector:
    matchLabels:
      app: snake-model
  template:
    metadata:
      labels:
        app: snake-model
    spec:
      containers:
      - name: vllm-server
        image: vllm/vllm-openai:latest
        ports:
        - containerPort: 8000
        env:
        - name: CUDA_VISIBLE_DEVICES
          value: "0"
        command: ["python", "-m", "vllm.entrypoints.openai.api_server"]
        args:
        - "--model=/models/snake-model"
        - "--tensor-parallel-size=1"
        - "--gpu-memory-utilization=0.85"
        - "--max-model-len=32768"
        - "--host=0.0.0.0"
        - "--port=8000"
        resources:
          requests:
            nvidia.com/gpu: 1
          limits:
            nvidia.com/gpu: 1
        volumeMounts:
        - name: model-storage
          mountPath: /models
      volumes:
      - name: model-storage
        persistentVolumeClaim:
          claimName: model-pvc
---
apiVersion: v1
kind: Service
metadata:
  name: snake-model-service
spec:
  selector:
    app: snake-model
  ports:
  - port: 80
    targetPort: 8000
  type: LoadBalancer
```

### Docker Compose
```yaml
# docker-compose.yml
version: '3.8'
services:
  snake-model-vllm:
    image: vllm/vllm-openai:latest
    ports:
      - "8000:8000"
    volumes:
      - ./models:/models
    command: >
      python -m vllm.entrypoints.openai.api_server
      --model /models/snake-model
      --tensor-parallel-size 1
      --gpu-memory-utilization 0.85
      --max-model-len 32768
      --host 0.0.0.0
      --port 8000
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  nginx-proxy:
    image: nginx:alpine
    ports:
      - "80:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
    depends_on:
      - snake-model-vllm
```

## ğŸ”§ é«˜çº§æŠ€æœ¯

### å¤§å‹æ¨¡å‹åˆ†ç‰‡
```python
def shard_large_model(model_path: str, output_dir: str, max_shard_size: str = "5GB"):
    """
    ä¸ºåˆ†å¸ƒå¼æ¨ç†åˆ†ç‰‡å¤§å‹æ¨¡å‹
    """
    from transformers import AutoModelForCausalLM
    
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        device_map="cpu",
        torch_dtype=torch.bfloat16
    )
    
    model.save_pretrained(
        output_dir,
        max_shard_size=max_shard_size,
        safe_serialization=True
    )
    
    print(f"âœ… æ¨¡å‹å·²åˆ†ç‰‡åˆ° {output_dir}")
```

### è‡ªå®šä¹‰åˆ†è¯å™¨ä¼˜åŒ–
```python
def optimize_tokenizer(tokenizer_path: str, output_path: str):
    """
    ä¸ºç‰¹å®šç”¨ä¾‹ä¼˜åŒ–åˆ†è¯å™¨
    """
    from transformers import AutoTokenizer
    
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)
    
    # ä¸ºè´ªåƒè›‡æ¸¸æˆæ·»åŠ ç‰¹æ®Š token
    special_tokens = [
        "<|game_state|>", "<|move|>", "<|strategy|>", 
        "<|food_pos|>", "<|snake_pos|>", "<|score|>"
    ]
    
    tokenizer.add_special_tokens({"additional_special_tokens": special_tokens})
    
    # ä¿å­˜ä¼˜åŒ–åçš„åˆ†è¯å™¨
    tokenizer.save_pretrained(output_path)
    
    print(f"âœ… åˆ†è¯å™¨å·²ä¼˜åŒ–å¹¶ä¿å­˜åˆ° {output_path}")
```

## ğŸš¨ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

#### CUDA å†…å­˜ä¸è¶³
```bash
# å‡å°‘æ‰¹å¤„ç†å¤§å°å’Œå†…å­˜ä½¿ç”¨
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
export CUDA_LAUNCH_BLOCKING=1

# å¯¹äº vLLM
python -m vllm.entrypoints.openai.api_server \
    --model /path/to/model \
    --gpu-memory-utilization 0.7 \
    --max-model-len 16384 \
    --tensor-parallel-size 2
```

#### æ¨¡å‹åŠ è½½é”™è¯¯
```python
# å¸¦é”™è¯¯å¤„ç†çš„å®‰å…¨æ¨¡å‹åŠ è½½
def safe_model_load(model_path: str):
    try:
        # é¦–å…ˆå°è¯•ä½¿ç”¨ trust_remote_code
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            trust_remote_code=True,
            torch_dtype=torch.bfloat16
        )
    except Exception as e:
        print(f"ä½¿ç”¨ trust_remote_code å¤±è´¥: {e}")
        # å›é€€åˆ°ä¸ä½¿ç”¨ trust_remote_code
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.bfloat16
        )
    return model
```

#### æ€§èƒ½ä¼˜åŒ–
```python
# æ€§èƒ½ç›‘æ§å’Œä¼˜åŒ–
def optimize_inference_performance():
    # å¯ç”¨ flash attention
    os.environ["FLASH_ATTENTION"] = "1"
    
    # ä¸º PyTorch 2.0+ å¯ç”¨ torch.compile
    torch._dynamo.config.suppress_errors = True
    
    # è®¾ç½®æœ€ä½³ CUDA è®¾ç½®
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True
```

## ğŸ“š å®Œæ•´å·¥ä½œæµè„šæœ¬

```bash
#!/bin/bash
# complete_deployment_pipeline.sh

set -e

# é…ç½®
BASE_MODEL="$1"
LORA_ADAPTER="$2"
OUTPUT_NAME="$3"
DEPLOYMENT_TYPE="${4:-vllm}"  # vllm, ollama, tgi

if [ -z "$BASE_MODEL" ] || [ -z "$LORA_ADAPTER" ] || [ -z "$OUTPUT_NAME" ]; then
    echo "ç”¨æ³•: $0 <base_model> <lora_adapter> <output_name> [deployment_type]"
    exit 1
fi

WORK_DIR="./deployment_pipeline"
MERGED_MODEL="$WORK_DIR/merged_models/$OUTPUT_NAME"
GGUF_MODEL="$WORK_DIR/gguf_models/$OUTPUT_NAME"

mkdir -p "$WORK_DIR"/{merged_models,gguf_models,deployments}

echo "ğŸš€ å¯åŠ¨å®Œæ•´éƒ¨ç½²æµæ°´çº¿..."

# æ­¥éª¤ 1: åˆå¹¶ LoRA
echo "ğŸ“¦ æ­¥éª¤ 1: åˆå¹¶ LoRA é€‚é…å™¨..."
python merge_lora.py \
    --base_model "$BASE_MODEL" \
    --lora_adapter "$LORA_ADAPTER" \
    --output_path "$MERGED_MODEL" \
    --merge_strategy linear \
    --dtype bfloat16

# æ­¥éª¤ 2: è½¬æ¢ä¸º GGUFï¼ˆå¯é€‰ï¼‰
if [ "$DEPLOYMENT_TYPE" = "ollama" ]; then
    echo "ğŸ”„ æ­¥éª¤ 2: è½¬æ¢ä¸º GGUF..."
    python convert_to_gguf.py \
        --model_path "$MERGED_MODEL" \
        --output_dir "$GGUF_MODEL" \
        --quantization Q4_K_M
    MODEL_PATH="$GGUF_MODEL"
else
    MODEL_PATH="$MERGED_MODEL"
fi

# æ­¥éª¤ 3: éƒ¨ç½²æ¨¡å‹
echo "ğŸš€ æ­¥éª¤ 3: ä½¿ç”¨ $DEPLOYMENT_TYPE éƒ¨ç½²æ¨¡å‹..."
case $DEPLOYMENT_TYPE in
    "vllm")
        python deploy_vllm.py \
            --model "$MODEL_PATH" \
            --host 0.0.0.0 \
            --port 8000 &
        ;;
    "ollama")
        ./deploy_ollama.sh "$MODEL_PATH" "$OUTPUT_NAME"
        ;;
    "tgi")
        ./deploy_tgi.sh "$MODEL_PATH" 8080 &
        ;;
esac

# æ­¥éª¤ 4: æµ‹è¯•éƒ¨ç½²
echo "ğŸ§ª æ­¥éª¤ 4: æµ‹è¯•éƒ¨ç½²..."
sleep 30  # ç­‰å¾…æœåŠ¡å™¨å¯åŠ¨
python test_model.py --api_url http://localhost:8000 --model_name "$OUTPUT_NAME"

echo "âœ… éƒ¨ç½²æµæ°´çº¿æˆåŠŸå®Œæˆï¼"
echo "ğŸ“Š æ¨¡å‹å¯ç”¨åœ°å€: http://localhost:8000"
echo "ğŸ“– æ–‡æ¡£åœ°å€: http://localhost:8000/docs"
```

## ğŸ¯ æœ€ä½³å®è·µæ€»ç»“

1. **å†…å­˜ç®¡ç†**: ä½¿ç”¨é€‚å½“çš„æ‰¹å¤„ç†å¤§å°å’Œå†…å­˜åˆ©ç”¨ç‡
2. **é‡åŒ–**: åœ¨è´¨é‡å’Œæ€§èƒ½ä¹‹é—´é€‰æ‹©æ­£ç¡®çš„å¹³è¡¡
3. **éƒ¨ç½²**: æ ¹æ®ç”¨ä¾‹å’Œè§„æ¨¡é€‰æ‹©éƒ¨ç½²æ–¹æ³•
4. **ç›‘æ§**: å®æ–½ç»¼åˆæ—¥å¿—è®°å½•å’Œç›‘æ§
5. **æµ‹è¯•**: åœ¨ç”Ÿäº§å‰å§‹ç»ˆéªŒè¯æ¨¡å‹æ€§èƒ½
6. **å®‰å…¨**: å®æ–½é€‚å½“çš„èº«ä»½éªŒè¯å’Œé€Ÿç‡é™åˆ¶
7. **æ‰©å±•**: è§„åˆ’è´Ÿè½½å‡è¡¡çš„æ°´å¹³æ‰©å±•

## ğŸ“ æ”¯æŒä¸èµ„æº

- **æ–‡æ¡£**: æŸ¥çœ‹æ¨¡å‹ç‰¹å®šæ–‡æ¡£
- **ç¤¾åŒº**: åŠ å…¥ vLLMã€Ollama å’Œ Transformers ç¤¾åŒº
- **é—®é¢˜**: å‘å„è‡ªçš„ GitHub ä»“åº“æŠ¥å‘Šé”™è¯¯
- **æ€§èƒ½**: ä½¿ç”¨åˆ†æå·¥å…·è¿›è¡Œä¼˜åŒ–

---

*æœ¬æŒ‡å—æä¾›äº†éƒ¨ç½²å¾®è°ƒè´ªåƒè›‡æ¸¸æˆæ¨¡å‹çš„å®Œæ•´æµæ°´çº¿ã€‚æ ¹æ®æ‚¨çš„ç‰¹å®šéœ€æ±‚å’Œç¡¬ä»¶èƒ½åŠ›è°ƒæ•´å‚æ•°å’Œé…ç½®ã€‚* 