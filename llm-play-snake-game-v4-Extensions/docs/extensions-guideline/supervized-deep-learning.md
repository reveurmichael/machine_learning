# Supervised Learning Extensions for Snake Game AI

> **Important — Authoritative Reference:** This document supplements the _Final Decision Series_ and extension guidelines. Supervised learning extensions follow the same architectural patterns established in the GOODFILES.

## 🎯 **Core Philosophy: Learning from Expert Demonstrations**

Supervised learning extensions demonstrate how ML models can learn optimal game strategies from datasets generated by heuristic algorithms, creating a bridge between deterministic algorithms and neural decision-making.

### **Design Philosophy**
- **Expert Demonstration Learning**: Learn from heuristic algorithm traces
- **Multi-Framework Support**: Neural networks, tree models, and ensemble methods
- **Factory Pattern Integration**: Consistent agent creation across model types
- **Educational Value**: Comprehensive ML model comparison framework

## 🏗️ **Extension Architecture Following GOODFILES**

### **Extension Evolution (v0.01 → v0.03)**
Following Final Decision 5 directory structure:

```
extensions/supervised-v0.02/
├── __init__.py                    # SupervisedConfig and factory registration
├── agents/                        # Model agent implementations (v0.02+)
│   ├── __init__.py               # Agent protocol and factory
│   ├── agent_mlp.py              # Multi-layer perceptron
│   ├── agent_cnn.py              # Convolutional neural network
│   ├── agent_lstm.py             # LSTM for sequential modeling
│   ├── agent_xgboost.py          # XGBoost classifier
│   ├── agent_lightgbm.py         # LightGBM classifier
│   └── agent_catboost.py         # CatBoost classifier
├── game_logic.py                  # ML prediction-based game logic
├── game_manager.py                # Training session management
├── game_data.py                   # ML-specific data tracking
└── scripts/                       # Training and evaluation scripts
    ├── train.py                  # CLI training interface
    └── evaluate.py               # Model evaluation
```

### **v0.03 Extensions**
```
extensions/supervised-v0.03/
├── [All v0.02 components]         # Agents folder copied exactly from v0.02
├── app.py                         # OOP Streamlit application
├── dashboard/                     # Modular UI components
│   ├── tab_training.py           # Model training interface
│   ├── tab_evaluation.py         # Performance evaluation
│   ├── tab_comparison.py         # Model comparison
│   └── tab_replay.py             # Model decision replay
└── scripts/                       # Enhanced CLI tools
    ├── train.py                  # Training pipeline
    ├── evaluate.py               # Evaluation pipeline
    └── replay.py                 # Model replay
```

## 🧠 **Model Categories and Architectures**

### **Neural Networks (PyTorch)**
Following the standardized agent naming from Final Decision 4:

```python
# agent_mlp.py → MLPAgent
class MLPAgent(BaseAgent):
    """Multi-layer perceptron for tabular feature learning"""
    def __init__(self, input_size: int = 16, hidden_layers: List[int] = [128, 64]):
        super().__init__()
        self.model = self._build_mlp_architecture(input_size, hidden_layers)
        
# agent_cnn.py → CNNAgent  
class CNNAgent(BaseAgent):
    """Convolutional neural network for spatial feature extraction"""
    def __init__(self, grid_size: int = 10, channels: int = 3):
        super().__init__()
        self.model = self._build_cnn_architecture(grid_size, channels)

# agent_lstm.py → LSTMAgent
class LSTMAgent(BaseAgent):
    """LSTM for sequential decision making with memory"""
    def __init__(self, sequence_length: int = 10, hidden_size: int = 128):
        super().__init__()
        self.model = self._build_lstm_architecture(sequence_length, hidden_size)
```

### **Tree-Based Models**
Following the same naming conventions:

```python
# agent_xgboost.py → XGBoostAgent
class XGBoostAgent(BaseAgent):
    """XGBoost classifier for structured tabular data"""
    def __init__(self, n_estimators: int = 100, max_depth: int = 6):
        super().__init__()
        self.model = xgb.XGBClassifier(n_estimators=n_estimators, max_depth=max_depth)

# agent_lightgbm.py → LightGBMAgent
class LightGBMAgent(BaseAgent):
    """LightGBM for fast gradient boosting with categorical features"""
    def __init__(self, n_estimators: int = 100, num_leaves: int = 31):
        super().__init__()
        self.model = lgb.LGBMClassifier(n_estimators=n_estimators, num_leaves=num_leaves)
```

## 🏭 **Factory Pattern Integration**

### **Supervised Agent Factory**
Following Final Decision 7-8 factory patterns:

```python
class SupervisedAgentFactory:
    """Factory for creating supervised learning agents"""
    
    _agent_registry = {
        "MLP": MLPAgent,
        "CNN": CNNAgent,
        "LSTM": LSTMAgent,
        "XGBOOST": XGBoostAgent,
        "LIGHTGBM": LightGBMAgent,
        "CATBOOST": CatBoostAgent,
    }
    
    @classmethod
    def create_agent(cls, model_type: str, **kwargs) -> BaseAgent:
        """Create agent by model type name"""
        agent_class = cls._agent_registry.get(model_type.upper())
        if not agent_class:
            raise ValueError(f"Unknown model type: {model_type}")
        return agent_class(**kwargs)
```

## 📊 **Dataset Integration**

### **Using Standardized CSV Schema**
Following Final Decision 2 configuration standards and csv-schema-1.md:

```python
from extensions.common.dataset_loader import load_dataset_for_training
from extensions.common.csv_schema import validate_dataset_compatibility

# Load datasets using standardized utilities
X_train, X_val, X_test, y_train, y_val, y_test, info = load_dataset_for_training(
    dataset_paths=[
        "logs/extensions/datasets/grid-size-10/heuristics_v0.03_timestamp/bfs/processed_data/tabular_data.csv",
        "logs/extensions/datasets/grid-size-10/heuristics_v0.03_timestamp/astar/processed_data/tabular_data.csv"
    ],
    grid_size=10
)
```

### **Multi-Format Support**
- **CSV**: Tabular data for tree models and MLPs
- **NPZ**: Sequential data for LSTM/GRU models  
- **Image tensors**: Board states for CNN models
- **Graph data**: Node/edge representations for GNN models

## 🚀 **Training Pipeline Architecture**

### **Unified Training Interface**
```python
class ModelTrainer:
    """Unified training interface for all model types"""
    
    def __init__(self, agent: BaseAgent, config: TrainingConfig):
        self.agent = agent
        self.config = config
        self.metrics_tracker = MetricsTracker()
        
    def train(self, train_data, val_data):
        """Generic training workflow"""
        for epoch in range(self.config.max_epochs):
            train_loss = self.train_epoch(train_data)
            val_metrics = self.validate(val_data)
            
            if self.early_stopping_criteria(val_metrics):
                break
                
        return self.agent
```

### **Model Persistence**
Following Final Decision 1 directory structure:

```python
# Models saved to standardized paths
model_path = get_model_path(
    extension_type="supervised",
    version="0.02", 
    grid_size=grid_size,
    algorithm="mlp",
    timestamp=timestamp
)
# Saves to: logs/extensions/models/grid-size-N/supervised_v0.02_timestamp/mlp/model_artifacts/
```

## 🎯 **Educational and Research Benefits**

### **Comparative ML Studies**
- **Neural vs Tree Models**: Performance comparison across different model families
- **Architecture Analysis**: Impact of network depth, width, and complexity
- **Data Efficiency**: Learning curves and sample complexity analysis
- **Transfer Learning**: Cross-grid-size model performance

### **Bridge to Advanced AI**
- **Foundation for RL**: Supervised models as behavior cloning baselines
- **LLM Integration**: Model outputs as training data for language model fine-tuning
- **Ensemble Methods**: Combining multiple model predictions for robust performance
- **Model Distillation**: Knowledge transfer from complex to simple models

---

**Supervised learning extensions provide a comprehensive framework for learning from expert demonstrations, demonstrating how different ML paradigms can be unified under consistent architectural patterns while maintaining the educational clarity and extensibility established in the Final Decision series.**