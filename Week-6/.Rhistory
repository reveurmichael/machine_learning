install.packages("ggplot")
# Part I - Exploratory data analysis
# Q1
# https://rafalab.github.io/pages/649/prostate.html
prostate<-read.table('prostate.data',header = TRUE)  # changer le chemin d'accÃ¨s aux donnÃ©es
# Q2
summary(prostate)
plot(prostate)
plot(prostate[,-10])
plot(prostate[,c(1,2,3,4,9)])
plot(prostate$lcavol,prostate$lpsa)
plot(prostate$lweight,prostate$lpsa)
plot(prostate$age,prostate$lpsa)
plot(prostate$svi,prostate$lpsa)
boxplot(lpsa~svi,data=prostate,xlab="svi",ylab="lpsa") # label
# Q3
library('FNN')
data<-prostate[,c('lcavol','lweight','age','lbph','lpsa','train')]
x.train<-scale(data[data$train==T,1:4])
y.train<-data[data$train==T,5]
x.tst<-scale(data[data$train==F,1:4])
y.tst<-data[data$train==F,5]
# https://www.cnblogs.com/listenfwind/p/10311496.html
reg<-knn.reg(train=x.train, test = x.tst, y=y.train, k = 5)
mean((y.tst-reg$pred)^2)
plot(y.tst,reg$pred,xlab='y',ylab='prediction')
abline(0,1)
# Q4
MSE<-rep(0,15)
for(k in 1:15){
reg<-knn.reg(train=x.train, test = x.tst,
y=y.train, k = k)
MSE[k]<-mean((y.tst-reg$pred)^2)
}
plot(1:15,MSE,type='b',xlab='k',ylab='MSE')
#-------------------------------------------------
# Part II
# Q1
library(mvtnorm)
mu1<-c(0,0)
mu2<-c(0,2)
mu3<-c(2,0)
Sigma1<-matrix(c(1,0.5,0.5,2),2,2)
Sigma2<-matrix(c(2,-0.5,-0.5,1),2,2)
Sigma3<-diag(c(1,1))
# Function to generate a data set
gen.data<- function(N,mu1,mu2,mu3,Sigma1,Sigma2,Sigma3,p1,p2){
y<-sample(3,N,prob=c(p1,p2,1-p1-p2),replace=TRUE)
X<-matrix(0,N,2)
N1<-length(which(y==1)) # number of objects from class 1
N2<-length(which(y==2))
N3<-length(which(y==3))
X[y==1,]<-rmvnorm(N1,mu1,Sigma1)
X[y==2,]<-rmvnorm(N2,mu2,Sigma2)
X[y==3,]<-rmvnorm(N3,mu3,Sigma3)
return(list(X=X,y=y))
}
# Training set
train<-gen.data(N=100,mu1,mu2,mu3,Sigma1,Sigma2,Sigma3,p1=0.3,p2=0.2)
plot(train$X[,1],train$X[,2],col=train$y,pch=train$y)
# Test set
test<-gen.data(N=1000,mu1,mu2,mu3,Sigma1,Sigma2,Sigma3,p1=0.3,p2=0.2)
# Q2-3
ypred<-knn(train$X,test$X,factor(train$y),k=5)
table(test$y,ypred)
err<-mean(test$y != ypred)
print(err)
# Q4
M<-10
Kmax<-20
ERR100<-matrix(0,M,Kmax)
ERR500<-ERR100
for(m in 1:M){
print(m)
train100<-gen.data(N=100,mu1,mu2,mu3,Sigma1,Sigma2,Sigma3,p1=0.3,p2=0.2)
train500<-gen.data(N=500,mu1,mu2,mu3,Sigma1,Sigma2,Sigma3,p1=0.3,p2=0.2)
for(k in 1:Kmax){
ypred<-knn(train100$X,test$X,factor(train100$y),k=k)
ERR100[m,k]<-mean(test$y != ypred)
ypred<-knn(train500$X,test$X,factor(train500$y),k=k)
ERR500[m,k]<-mean(test$y != ypred)
}
}
err100<-colMeans(ERR100)
err500<-colMeans(ERR500)
plot(1:Kmax,err100,type="b",ylim=range(err100,err500))
lines(1:Kmax,err500,col="red")
install.packages("mvtnorm")
install.packages("mvtnorm")
# Part I - Exploratory data analysis
# Q1
# https://rafalab.github.io/pages/649/prostate.html
prostate<-read.table('prostate.data',header = TRUE)  # changer le chemin d'accÃ¨s aux donnÃ©es
# Q2
summary(prostate)
plot(prostate)
plot(prostate[,-10])
plot(prostate[,c(1,2,3,4,9)])
plot(prostate$lcavol,prostate$lpsa)
plot(prostate$lweight,prostate$lpsa)
plot(prostate$age,prostate$lpsa)
plot(prostate$svi,prostate$lpsa)
boxplot(lpsa~svi,data=prostate,xlab="svi",ylab="lpsa") # label
# Q3
library('FNN')
data<-prostate[,c('lcavol','lweight','age','lbph','lpsa','train')]
x.train<-scale(data[data$train==T,1:4])
y.train<-data[data$train==T,5]
x.tst<-scale(data[data$train==F,1:4])
y.tst<-data[data$train==F,5]
# https://www.cnblogs.com/listenfwind/p/10311496.html
reg<-knn.reg(train=x.train, test = x.tst, y=y.train, k = 5)
mean((y.tst-reg$pred)^2)
plot(y.tst,reg$pred,xlab='y',ylab='prediction')
abline(0,1)
# Q4
MSE<-rep(0,15)
for(k in 1:15){
reg<-knn.reg(train=x.train, test = x.tst,
y=y.train, k = k)
MSE[k]<-mean((y.tst-reg$pred)^2)
}
plot(1:15,MSE,type='b',xlab='k',ylab='MSE')
#-------------------------------------------------
# Part II
# Q1
library(mvtnorm)
mu1<-c(0,0)
mu2<-c(0,2)
mu3<-c(2,0)
Sigma1<-matrix(c(1,0.5,0.5,2),2,2)
Sigma2<-matrix(c(2,-0.5,-0.5,1),2,2)
Sigma3<-diag(c(1,1))
# Function to generate a data set
gen.data<- function(N,mu1,mu2,mu3,Sigma1,Sigma2,Sigma3,p1,p2){
y<-sample(3,N,prob=c(p1,p2,1-p1-p2),replace=TRUE)
X<-matrix(0,N,2)
N1<-length(which(y==1)) # number of objects from class 1
N2<-length(which(y==2))
N3<-length(which(y==3))
X[y==1,]<-rmvnorm(N1,mu1,Sigma1)
X[y==2,]<-rmvnorm(N2,mu2,Sigma2)
X[y==3,]<-rmvnorm(N3,mu3,Sigma3)
return(list(X=X,y=y))
}
# Training set
train<-gen.data(N=100,mu1,mu2,mu3,Sigma1,Sigma2,Sigma3,p1=0.3,p2=0.2)
plot(train$X[,1],train$X[,2],col=train$y,pch=train$y)
# Test set
test<-gen.data(N=1000,mu1,mu2,mu3,Sigma1,Sigma2,Sigma3,p1=0.3,p2=0.2)
# Q2-3
ypred<-knn(train$X,test$X,factor(train$y),k=5)
table(test$y,ypred)
err<-mean(test$y != ypred)
print(err)
# Q4
M<-10
Kmax<-20
ERR100<-matrix(0,M,Kmax)
ERR500<-ERR100
for(m in 1:M){
print(m)
train100<-gen.data(N=100,mu1,mu2,mu3,Sigma1,Sigma2,Sigma3,p1=0.3,p2=0.2)
train500<-gen.data(N=500,mu1,mu2,mu3,Sigma1,Sigma2,Sigma3,p1=0.3,p2=0.2)
for(k in 1:Kmax){
ypred<-knn(train100$X,test$X,factor(train100$y),k=k)
ERR100[m,k]<-mean(test$y != ypred)
ypred<-knn(train500$X,test$X,factor(train500$y),k=k)
ERR500[m,k]<-mean(test$y != ypred)
}
}
err100<-colMeans(ERR100)
err500<-colMeans(ERR500)
plot(1:Kmax,err100,type="b",ylim=range(err100,err500))
lines(1:Kmax,err500,col="red")
install.packages("FNN")
# Part I - Exploratory data analysis
# Q1
# https://rafalab.github.io/pages/649/prostate.html
prostate<-read.table('prostate.data',header = TRUE)  # changer le chemin d'accÃ¨s aux donnÃ©es
# Q2
summary(prostate)
plot(prostate)
plot(prostate[,-10])
plot(prostate[,c(1,2,3,4,9)])
plot(prostate$lcavol,prostate$lpsa)
plot(prostate$lweight,prostate$lpsa)
plot(prostate$age,prostate$lpsa)
plot(prostate$svi,prostate$lpsa)
boxplot(lpsa~svi,data=prostate,xlab="svi",ylab="lpsa") # label
# Q3
library('FNN')
data<-prostate[,c('lcavol','lweight','age','lbph','lpsa','train')]
x.train<-scale(data[data$train==T,1:4])
y.train<-data[data$train==T,5]
x.tst<-scale(data[data$train==F,1:4])
y.tst<-data[data$train==F,5]
# https://www.cnblogs.com/listenfwind/p/10311496.html
reg<-knn.reg(train=x.train, test = x.tst, y=y.train, k = 5)
mean((y.tst-reg$pred)^2)
plot(y.tst,reg$pred,xlab='y',ylab='prediction')
abline(0,1)
# Q4
MSE<-rep(0,15)
for(k in 1:15){
reg<-knn.reg(train=x.train, test = x.tst,
y=y.train, k = k)
MSE[k]<-mean((y.tst-reg$pred)^2)
}
plot(1:15,MSE,type='b',xlab='k',ylab='MSE')
#-------------------------------------------------
# Part II
# Q1
library(mvtnorm)
mu1<-c(0,0)
mu2<-c(0,2)
mu3<-c(2,0)
Sigma1<-matrix(c(1,0.5,0.5,2),2,2)
Sigma2<-matrix(c(2,-0.5,-0.5,1),2,2)
Sigma3<-diag(c(1,1))
# Function to generate a data set
gen.data<- function(N,mu1,mu2,mu3,Sigma1,Sigma2,Sigma3,p1,p2){
y<-sample(3,N,prob=c(p1,p2,1-p1-p2),replace=TRUE)
X<-matrix(0,N,2)
N1<-length(which(y==1)) # number of objects from class 1
N2<-length(which(y==2))
N3<-length(which(y==3))
X[y==1,]<-rmvnorm(N1,mu1,Sigma1)
X[y==2,]<-rmvnorm(N2,mu2,Sigma2)
X[y==3,]<-rmvnorm(N3,mu3,Sigma3)
return(list(X=X,y=y))
}
# Training set
train<-gen.data(N=100,mu1,mu2,mu3,Sigma1,Sigma2,Sigma3,p1=0.3,p2=0.2)
plot(train$X[,1],train$X[,2],col=train$y,pch=train$y)
# Test set
test<-gen.data(N=1000,mu1,mu2,mu3,Sigma1,Sigma2,Sigma3,p1=0.3,p2=0.2)
# Q2-3
ypred<-knn(train$X,test$X,factor(train$y),k=5)
table(test$y,ypred)
err<-mean(test$y != ypred)
print(err)
# Q4
M<-10
Kmax<-20
ERR100<-matrix(0,M,Kmax)
ERR500<-ERR100
for(m in 1:M){
print(m)
train100<-gen.data(N=100,mu1,mu2,mu3,Sigma1,Sigma2,Sigma3,p1=0.3,p2=0.2)
train500<-gen.data(N=500,mu1,mu2,mu3,Sigma1,Sigma2,Sigma3,p1=0.3,p2=0.2)
for(k in 1:Kmax){
ypred<-knn(train100$X,test$X,factor(train100$y),k=k)
ERR100[m,k]<-mean(test$y != ypred)
ypred<-knn(train500$X,test$X,factor(train500$y),k=k)
ERR500[m,k]<-mean(test$y != ypred)
}
}
err100<-colMeans(ERR100)
err500<-colMeans(ERR500)
plot(1:Kmax,err100,type="b",ylim=range(err100,err500))
lines(1:Kmax,err500,col="red")
?c()
?c
?install
??install
# Part I - Exploratory data analysis
# Q1
# https://rafalab.github.io/pages/649/prostate.html
prostate<-read.table('prostate.data',header = TRUE)  # changer le chemin d'accÃ¨s aux donnÃ©es
# Q2
summary(prostate)
plot(prostate)
plot(prostate[,-10])
plot(prostate[,c(1,2,3,4,9)])
plot(prostate$lcavol,prostate$lpsa)
plot(prostate$lweight,prostate$lpsa)
plot(prostate$age,prostate$lpsa)
plot(prostate$svi,prostate$lpsa)
boxplot(lpsa~svi,data=prostate,xlab="svi",ylab="lpsa") # label
# Q3
library('FNN')
data<-prostate[,c('lcavol','lweight','age','lbph','lpsa','train')]
x.train<-scale(data[data$train==T,1:4])
y.train<-data[data$train==T,5]
x.test<-scale(data[data$train==F,1:4])
y.test<-data[data$train==F,5]
# https://www.cnblogs.com/listenfwind/p/10311496.html
reg<-knn.reg(train=x.train, test = x.test, y=y.train, k = 5)
mean((y.test-reg$pred)^2)
plot(y.test,reg$pred,xlab='y',ylab='prediction')
abline(0,1)
# Q4
MSE<-rep(0,15)
for(k in 1:15){
reg<-knn.reg(train=x.train, test = x.test,
y=y.train, k = k)
MSE[k]<-mean((y.test-reg$pred)^2)
}
plot(1:15,MSE,type='b',xlab='k',ylab='MSE')
#-------------------------------------------------
# Part II
# Q1
library(mvtnorm)
mu1<-c(0,0)
mu2<-c(0,2)
mu3<-c(2,0)
Sigma1<-matrix(c(1,0.5,0.5,2),2,2)
Sigma2<-matrix(c(2,-0.5,-0.5,1),2,2)
Sigma3<-diag(c(1,1))
# Function to generate a data set
gen.data<- function(N,mu1,mu2,mu3,Sigma1,Sigma2,Sigma3,p1,p2){
y<-sample(3,N,prob=c(p1,p2,1-p1-p2),replace=TRUE)
X<-matrix(0,N,2)
N1<-length(which(y==1)) # number of objects from class 1
N2<-length(which(y==2))
N3<-length(which(y==3))
X[y==1,]<-rmvnorm(N1,mu1,Sigma1)
X[y==2,]<-rmvnorm(N2,mu2,Sigma2)
X[y==3,]<-rmvnorm(N3,mu3,Sigma3)
return(list(X=X,y=y))
}
# Training set
train<-gen.data(N=100,mu1,mu2,mu3,Sigma1,Sigma2,Sigma3,p1=0.3,p2=0.2)
plot(train$X[,1],train$X[,2],col=train$y,pch=train$y)
# Test set
test<-gen.data(N=1000,mu1,mu2,mu3,Sigma1,Sigma2,Sigma3,p1=0.3,p2=0.2)
# Q2-3
ypred<-knn(train$X,test$X,factor(train$y),k=5)
table(test$y,ypred)
err<-mean(test$y != ypred)
print(err)
# Q4
M<-10
Kmax<-20
ERR100<-matrix(0,M,Kmax)
ERR500<-ERR100
for(m in 1:M){
print(m)
train100<-gen.data(N=100,mu1,mu2,mu3,Sigma1,Sigma2,Sigma3,p1=0.3,p2=0.2)
train500<-gen.data(N=500,mu1,mu2,mu3,Sigma1,Sigma2,Sigma3,p1=0.3,p2=0.2)
for(k in 1:Kmax){
ypred<-knn(train100$X,test$X,factor(train100$y),k=k)
ERR100[m,k]<-mean(test$y != ypred)
ypred<-knn(train500$X,test$X,factor(train500$y),k=k)
ERR500[m,k]<-mean(test$y != ypred)
}
}
err100<-colMeans(ERR100)
err500<-colMeans(ERR500)
plot(1:Kmax,err100,type="b",ylim=range(err100,err500))
lines(1:Kmax,err500,col="red")
rm(ls())
rm(list=ls())
clear
clear()
# Q1
# https://rafalab.github.io/pages/649/prostate.html
prostate<-read.table('prostate.data',header = TRUE)
# Q1
# https://rafalab.github.io/pages/649/prostate.html
prostate<-read.table('prostate.data',header = TRUE)
pwd()
library(glmnet)
library(leaps)
library(MASS)
library(naivebayes)
library(nnet)
# Part I: prostate data
prostate<-read.table('prostate.data',header = TRUE)
summary(prostate)
train<-prostate$train
prostate<-prostate[,-10]
xtst<-as.matrix(prostate[train==FALSE,1:8])
ntst<-nrow(xtst)
X<-cbind(rep(1,ntst),xtst)
ytst<-prostate$lpsa[train==FALSE]
setwd("C:/Users/Administrator/ML01_2021_Spring/Week-6")
library(glmnet)
library(leaps)
library(MASS)
library(naivebayes)
library(nnet)
# Part I: prostate data
prostate<-read.table('prostate.data',header = TRUE)
summary(prostate)
train<-prostate$train
prostate<-prostate[,-10]
xtst<-as.matrix(prostate[train==FALSE,1:8])
ntst<-nrow(xtst)
X<-cbind(rep(1,ntst),xtst)
ytst<-prostate$lpsa[train==FALSE]
library(glmnet)
library(leaps)
library(MASS)
library(naivebayes)
library(nnet)
# Part I: prostate data
prostate<-read.table('prostate.data',header = TRUE)
summary(prostate)
train<-prostate$train
prostate<-prostate[,-10]
xtst<-as.matrix(prostate[train==FALSE,1:8])
ntst<-nrow(xtst)
X<-cbind(rep(1,ntst),xtst)
ytst<-prostate$lpsa[train==FALSE]
library(glmnet)
library(leaps)
library(MASS)
library(naivebayes)
library(nnet)
prostate<-read.table('prostate.data',header = TRUE)
summary(prostate)
train<-prostate$train
prostate<-prostate[,-10]
xtst<-as.matrix(prostate[train==FALSE,1:8])
ntst<-nrow(xtst)
X<-cbind(rep(1,ntst),xtst)
ytst<-prostate$lpsa[train==FALSE]
# Linear regression with all predictors
reg<- lm(lpsa ~.  ,data=prostate[train==TRUE,])
summary(reg)
pred<-predict(reg,newdata=prostate[train==FALSE,])
mse_full<-mean((ytst-pred)^2)
reg.forward<-regsubsets(lpsa~.,data=prostate[train==TRUE,],
method='forward',nvmax=30)
plot(reg.forward,scale="bic")
res<-summary(reg.forward)
# BIC
best<-which.min(res$bic)
ypred<-X[,res$which[best,]]%*%coef(reg.forward,best)
mse_forward_bic<-mean((ypred-ytst)^2)
plot(reg.forward,scale="adjr2")
best<-which.max(res$adjr2)
ypred<-X[,res$which[best,]]%*%coef(reg.forward,best)
mse_forward_adjr2<-mean((ypred-ytst)^2)
reg.backward<-regsubsets(lpsa~.,data=prostate[train==TRUE,],method='backward',nvmax=30)
plot(reg.backward,scale="bic")
res<-summary(reg.backward)
reg.exhaustive<-regsubsets(lpsa~.,data=prostate[train==TRUE,],method='exhaustive',nvmax=30)
plot(reg.exhaustive,scale="bic")
res<-summary(reg.exhaustive)
# Again the results are the same
# Backward selection
reg.backward<-regsubsets(lpsa~.,data=prostate[train==TRUE,],method='backward',nvmax=30)
plot(reg.backward,scale="bic")
res<-summary(reg.backward)
# The results are the same as those of forward selection
# Optimal subset
reg.exhaustive<-regsubsets(lpsa~.,data=prostate[train==TRUE,],method='exhaustive',nvmax=30)
plot(reg.exhaustive,scale="bic")
res<-summary(reg.exhaustive)
# Again the results are the same
# Ridge
xapp<-as.matrix(prostate[train==TRUE,1:8])
yapp<-prostate$lpsa[train==TRUE]
cv.out<-cv.glmnet(xapp,yapp,alpha=0,standardize=TRUE)
plot(cv.out)
fit<-glmnet(xapp,yapp,lambda=cv.out$lambda.min,alpha=0,standardize=TRUE)
ridge.pred<-predict(fit,s=cv.out$lambda.min,newx=xtst)
mse_ridge<-mean((ytst-ridge.pred)^2)
cv.out<-cv.glmnet(xapp,yapp,alpha=1,standardize=TRUE)
plot(cv.out)
fit<-glmnet(xapp,yapp,lambda=cv.out$lambda.min,alpha=1,standardize=TRUE)
lasso.pred<-predict(fit,s=cv.out$lambda.min,newx=xtst)
mse_lasso<-mean((ytst-lasso.pred)^2)
print(c(mse_full,mse_forward_bic,mse_forward_adjr2, mse_ridge,mse_lasso))
vowel <- read.table('vowel.data',
header=FALSE)
names(vowel)[11]<-'class'
n<-nrow(vowel)
ntrain<-round(2*n/3)
ntest<-n-ntrain
train<-sample(n,ntrain)
vowel.train<-vowel[train,]
vowel.test<-vowel[-train,]
K<-5
folds=sample(1:K,ntrain,replace=TRUE)
CV<-matrix(0,K,4)
for(k in (1:K)){
fit.lda<- lda(class~.,data=vowel.train[folds!=k,])
pred.lda<-predict(fit.lda,newdata=vowel.train[folds==k,])
CV[k,1]<-sum(pred.lda$class!=vowel.train$class[folds==k])
fit.qda<- qda(class~.,data=vowel.train[folds!=k,])
pred.qda<-predict(fit.qda,newdata=vowel.train[folds==k,])
CV[k,2]<-sum(pred.qda$class!=vowel.train$class[folds==k])
fit.nb<- naive_bayes(as.factor(class)~.,data=vowel.train[folds!=k,])
pred.nb<-predict(fit.nb,newdata=vowel.train[folds==k,],type="class")
CV[k,3]<-sum(pred.nb!=vowel.train$class[folds==k])
fit.logreg<- multinom(as.factor(class)~.,data=vowel.train[folds!=k,],trace=FALSE)
pred.logreg<-predict(fit.logreg,newdata=vowel.train[folds==k,],type='class')
CV[k,4]<-sum(pred.logreg!=vowel.train$class[folds==k])
}
err_cv=colSums(CV)/ntrain
print(err_cv)
fit.qda<- qda(class~.,data=vowel.train)
pred.qda<-predict(fit.qda,newdata=vowel.test)
err<- mean(pred.qda$class!=vowel.test$class)
err
K<-5
folds=sample(1:K,ntrain,replace=TRUE)
fols
folds
