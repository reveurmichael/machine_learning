# Supervised Learning for Snake Game AI

> **Important â€” Authoritative Reference:** This document supplements the _Final Decision Series_ (`final-decision-0.md` â†’ `final-decision-10.md`) and defines supervised learning patterns for extensions.

> **Guidelines Alignment:**
> - This document is governed by the guidelines in `final-decision-10.md`.
> - All extension factories must use the canonical method name `create()` (never `create_agent`, `create_model`, etc.).
> - All code must use simple print logging (simple logging).
> - Reference: `extensions/common/utils/factory_utils.py` for the canonical `SimpleFactory` implementation.
> - This file follows KEEP_THOSE_MARKDOWN_FILES_SIMPLE_RULES (target 300-500 lines).

> **See also:** `agents.md`, `core.md`, `final-decision-10.md`, `factory-design-pattern.md`, `config.md`, `csv-schema-1.md`, `tree-models.md`.

# Supervised Learning for Snake Game AI

## ðŸŽ¯ **Core Philosophy: Learning from Expert Demonstrations + SUPREME_RULES**

Supervised learning enables agents to learn optimal Snake game strategies by training on high-quality datasets generated by heuristic algorithms. **This extension strictly follows the SUPREME_RULES** established in `final-decision-10.md`, particularly the **canonical `create()` method patterns and simple logging requirements** for all learning-based systems.

### **Guidelines Alignment**
- **final-decision-10.md Guideline 1**: Follows all established GOOD_RULES patterns for supervised learning architectures
- **final-decision-10.md Guideline 2**: Uses precise `final-decision-N.md` format consistently throughout supervised implementations
- **simple logging**: Lightweight, OOP-based common utilities with simple logging (print() statements only)

### **Educational Value**
- **Expert Knowledge Transfer**: Learn how to distill expert strategies using canonical patterns
- **Model Training**: Understand supervised learning with simple logging throughout
- **Feature Engineering**: Experience data preprocessing following SUPREME_RULES compliance
- **Performance Evaluation**: See canonical patterns in model evaluation systems

## ðŸ—ï¸ **Extension Structure**

### **Directory Layout**
```
extensions/supervised-v0.02/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ __init__.py               # Agent factory
â”‚   â”œâ”€â”€ agent_mlp.py              # Multi-layer Perceptron
â”‚   â”œâ”€â”€ agent_cnn.py              # Convolutional Neural Network
â”‚   â”œâ”€â”€ agent_lstm.py             # Long Short-Term Memory
â”‚   â””â”€â”€ agent_xgboost.py          # XGBoost (tree-based)
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ neural_networks.py        # Neural network architectures
â”‚   â”œâ”€â”€ tree_models.py            # Tree-based models
â”‚   â””â”€â”€ model_manager.py          # Model management
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ trainer.py                # Training pipeline
â”‚   â””â”€â”€ data_loader.py            # Data loading utilities
â”œâ”€â”€ game_logic.py                 # Supervised game logic
â”œâ”€â”€ game_manager.py               # Supervised manager
â””â”€â”€ main.py                       # CLI interface
```

## ðŸ”§ **Implementation Patterns**

### **Supervised Agent Factory (SUPREME_RULES Compliant)**
**CRITICAL REQUIREMENT**: All supervised learning factories MUST use the canonical `create()` method exactly as specified in `final-decision-10.md` SUPREME_RULES:

```python
class SupervisedAgentFactory:
    """
    Factory Pattern for Supervised Learning agents following final-decision-10.md SUPREME_RULES
    
    Design Pattern: Factory Pattern (Canonical Implementation)
    Purpose: Demonstrates canonical create() method for supervised AI agents
    Educational Value: Shows how SUPREME_RULES apply to advanced AI systems -
    canonical patterns work regardless of AI complexity.
    
    Reference: final-decision-10.md SUPREME_RULES for canonical method naming
    """
    
    _registry = {
        "MLP": MLPAgent,
        "CNN": CNNAgent,
        "LSTM": LSTMAgent,
        "XGBOOST": XGBoostAgent,
        "RANDOM_FOREST": RandomForestAgent,
    }
    
    @classmethod
    def create(cls, algorithm_type: str, **kwargs):  # CANONICAL create() method - SUPREME_RULES
        """Create Supervised agent using canonical create() method following final-decision-10.md"""
        agent_class = cls._registry.get(algorithm_type.upper())
        if not agent_class:
            available = list(cls._registry.keys())
            raise ValueError(f"Unknown Supervised algorithm: {algorithm_type}. Available: {available}")
        print(f"[SupervisedAgentFactory] Creating agent: {algorithm_type}")  # Simple logging - SUPREME_RULES
        return agent_class(**kwargs)

# âŒ FORBIDDEN: Non-canonical method names (violates SUPREME_RULES)
class SupervisedAgentFactory:
    def create_supervised_agent(self, algorithm_type: str):  # FORBIDDEN - not canonical
        pass
    
    def build_ml_model(self, algorithm_type: str):  # FORBIDDEN - not canonical
        pass
    
    def make_supervised_algorithm(self, algorithm_type: str):  # FORBIDDEN - not canonical
        pass
```

### **MLP Agent Implementation**
```python
class MLPAgent(BaseAgent):
    """
    Multi-layer Perceptron agent for Snake game
    
    Design Pattern: Strategy Pattern
    - Encapsulates MLP learning algorithm
    - Uses 16-feature tabular data format
    - Provides interpretable decision making
    """
    
    def __init__(self, name: str, grid_size: int):
        super().__init__(name, grid_size)
        self.model = None
        self.feature_names = [
            'head_x', 'head_y', 'apple_x', 'apple_y',
            'snake_length', 'apple_dir_up', 'apple_dir_down',
            'apple_dir_left', 'apple_dir_right', 'danger_straight',
            'danger_left', 'danger_right', 'free_space_up',
            'free_space_down', 'free_space_left', 'free_space_right'
        ]
        print(f"[MLPAgent] Initialized MLP agent: {name}")
    
    def plan_move(self, game_state: Dict[str, Any]) -> str:
        """Plan move using MLP model"""
        if self.model is None:
            print("[MLPAgent] Model not trained, using random move")
            return random.choice(['UP', 'DOWN', 'LEFT', 'RIGHT'])
        
        features = self._extract_features(game_state)
        prediction = self.model.predict([features])[0]
        print(f"[MLPAgent] Predicted move: {prediction}")
        return prediction
```

## ðŸ“Š **Data Integration**

### **Dataset Loading**
```python
from extensions.common.dataset_utils import load_dataset_for_training

# Load training data from heuristics-v0.04
X_train, X_val, X_test, y_train, y_val, y_test, info = load_dataset_for_training(
    dataset_paths=["path/to/heuristics_v0.04_dataset.csv"],
    grid_size=grid_size
)
```

### **Feature Engineering**
```python
class FeatureExtractor:
    """
    Extract features from game state for supervised learning
    
    Design Pattern: Adapter Pattern
    - Adapts game state to feature vector format
    - Maintains consistent feature representation
    - Enables easy integration with different models
    """
    
    def __init__(self, grid_size: int):
        self.grid_size = grid_size
        print(f"[FeatureExtractor] Initialized for {grid_size}x{grid_size} grid")
    
    def extract_features(self, game_state: Dict[str, Any]) -> List[float]:
        """Extract 16 standardized features from game state"""
        features = []
        
        # Position features
        features.extend([
            game_state['snake_head'][0],  # head_x
            game_state['snake_head'][1],  # head_y
            game_state['apple_position'][0],  # apple_x
            game_state['apple_position'][1],  # apple_y
        ])
        
        # Game state features
        features.append(len(game_state['snake_body']))  # snake_length
        
        # Direction features (calculated)
        direction_features = self._calculate_direction_features(game_state)
        features.extend(direction_features)
        
        # Danger features (calculated)
        danger_features = self._calculate_danger_features(game_state)
        features.extend(danger_features)
        
        # Free space features (calculated)
        free_space_features = self._calculate_free_space_features(game_state)
        features.extend(free_space_features)
        
        print(f"[FeatureExtractor] Extracted {len(features)} features")
        return features
```

## ðŸš€ **Advanced Features**

### **CNN for Spatial Learning**
```python
class CNNAgent(BaseAgent):
    """
    Convolutional Neural Network agent for spatial patterns
    
    Design Pattern: Strategy Pattern
    - Uses 2D spatial representation of game state
    - Learns spatial patterns and relationships
    - Maintains grid structure information
    """
    
    def __init__(self, name: str, grid_size: int):
        super().__init__(name, grid_size)
        self.model = self._build_cnn_model()
        print(f"[CNNAgent] Initialized CNN agent: {name}")
    
    def plan_move(self, game_state: Dict[str, Any]) -> str:
        """Plan move using CNN model"""
        if self.model is None:
            print("[CNNAgent] Model not trained, using random move")
            return random.choice(['UP', 'DOWN', 'LEFT', 'RIGHT'])
        
        # Convert to 2D spatial representation
        spatial_state = self._game_state_to_2d(game_state)
        prediction = self.model.predict(spatial_state)
        move = ['UP', 'DOWN', 'LEFT', 'RIGHT'][np.argmax(prediction)]
        print(f"[CNNAgent] Predicted move: {move}")
        return move
```

### **LSTM for Temporal Learning**
```python
class LSTMAgent(BaseAgent):
    """
    Long Short-Term Memory agent for temporal patterns
    
    Design Pattern: Strategy Pattern
    - Uses sequential state representation
    - Learns temporal dependencies and patterns
    - Maintains memory of past game states
    """
    
    def __init__(self, name: str, grid_size: int):
        super().__init__(name, grid_size)
        self.model = self._build_lstm_model()
        self.state_history = []
        print(f"[LSTMAgent] Initialized LSTM agent: {name}")
    
    def plan_move(self, game_state: Dict[str, Any]) -> str:
        """Plan move using LSTM model"""
        if self.model is None:
            print("[LSTMAgent] Model not trained, using random move")
            return random.choice(['UP', 'DOWN', 'LEFT', 'RIGHT'])
        
        # Update state history
        features = self._extract_features(game_state)
        self.state_history.append(features)
        
        # Keep only recent history
        if len(self.state_history) > 10:
            self.state_history = self.state_history[-10:]
        
        # Predict using temporal sequence
        sequence = np.array([self.state_history])
        prediction = self.model.predict(sequence)
        move = ['UP', 'DOWN', 'LEFT', 'RIGHT'][np.argmax(prediction)]
        print(f"[LSTMAgent] Predicted move: {move}")
        return move
```

## ðŸ“‹ **Configuration and Usage**

### **Supervised Learning Configuration**
```python
SUPERVISED_CONFIG = {
    'mlp': {
        'hidden_layers': [128, 64, 32],
        'learning_rate': 0.001,
        'batch_size': 32,
        'epochs': 100
    },
    'cnn': {
        'filters': [32, 64, 128],
        'kernel_size': 3,
        'learning_rate': 0.001,
        'batch_size': 16,
        'epochs': 50
    },
    'lstm': {
        'units': 64,
        'sequence_length': 10,
        'learning_rate': 0.001,
        'batch_size': 32,
        'epochs': 100
    },
    'xgboost': {
        'n_estimators': 100,
        'max_depth': 6,
        'learning_rate': 0.1
    }
}
```

### **Training Commands**
```bash
python main.py --model MLP --dataset heuristics-v0.04 --epochs 100
python main.py --model CNN --dataset heuristics-v0.04 --epochs 50
python main.py --model LSTM --dataset heuristics-v0.04 --epochs 100
python main.py --model XGBOOST --dataset heuristics-v0.04
```

## ðŸ”— **Integration with Other Extensions**

### **With Heuristics Extensions**
- Use heuristics-v0.04 CSV datasets for training
- Compare supervised performance with algorithmic approaches
- Analyze learned patterns vs. algorithmic logic

### **With Reinforcement Learning**
- Supervised learning provides initialization for RL
- Compare imitation learning vs. exploration-based learning
- Study transfer learning from supervised to RL

## ðŸŽ“ **Educational Applications**

### **Model Types**
- **Neural Networks**: Deep learning for complex patterns
- **Tree Models**: Interpretable decision making
- **Sequential Models**: Temporal pattern recognition
- **Spatial Models**: Grid-based pattern learning

### **Performance Analysis**
- **Accuracy**: Prediction accuracy on test data
- **Generalization**: Performance on unseen scenarios
- **Interpretability**: Understanding learned patterns
- **Efficiency**: Training and inference speed

---

**Supervised learning provides powerful pattern recognition capabilities for Snake game AI, enabling agents to learn from expert demonstrations while maintaining full compliance with established GOOD_RULES standards.**