# Eureka: Reward Function Evolution for Snake Game AI

> **Important ‚Äî Authoritative Reference:** This document supplements the _Final Decision Series_ (`final-decision-0.md` ‚Üí `final-decision-10.md`) and follows established architectural patterns.

## üéØ **Core Philosophy: Automated Reward Engineering**

Eureka represents a paradigm shift in reinforcement learning where reward functions are automatically generated and evolved using large language models (LLMs) rather than being hand-crafted by humans. This approach can discover novel and effective reward shaping strategies that might not be intuitive to human designers.

### **Design Philosophy**
- **Automated Discovery**: Let AI discover optimal reward functions
- **Human-AI Collaboration**: Combine human insights with AI exploration
- **Iterative Refinement**: Continuous improvement through evolutionary processes
- **Educational Value**: Demonstrate automated engineering principles

## üß† **Eureka vs. Traditional Approaches**

### **Traditional Reinforcement Learning**
```python
# ‚ùå Hand-crafted reward function
def traditional_reward(game_state):
    reward = 0
    if game_state.apple_eaten:
        reward += 10  # Human intuition
    if game_state.game_over:
        reward -= 100  # Human penalty design
    reward += 0.1 * game_state.score  # Linear human assumption
    return reward
```

### **Eureka Approach**
```python
# ‚úÖ LLM-generated and evolved reward function
def evolved_reward(game_state):
    # Generated by LLM based on performance feedback
    reward = 0
    
    # Discovered patterns that humans might miss
    if game_state.apple_eaten:
        # Non-linear reward based on snake length
        reward += 10 * (1 + 0.1 * game_state.snake_length)
    
    # Sophisticated survival incentives
    if game_state.near_wall_count < 2:
        reward += 2  # Safety reward
    
    # Efficiency bonuses discovered through evolution
    if game_state.manhattan_distance_to_apple < previous_distance:
        reward += 1  # Progress reward
    
    return reward
```

## üèóÔ∏è **Eureka Architecture for Snake Game**

### **Extension Structure**
```
extensions/eureka-v0.02/
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ agents/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py               # Agent factory
‚îÇ   ‚îú‚îÄ‚îÄ agent_eureka_dqn.py       # DQN with evolved rewards
‚îÇ   ‚îú‚îÄ‚îÄ agent_eureka_ppo.py       # PPO with evolved rewards
‚îÇ   ‚îî‚îÄ‚îÄ agent_reward_tester.py    # Test reward functions
‚îú‚îÄ‚îÄ reward_evolution/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ reward_generator.py       # LLM-based reward generation
‚îÇ   ‚îú‚îÄ‚îÄ reward_evaluator.py       # Reward function evaluation
‚îÇ   ‚îú‚îÄ‚îÄ population_manager.py     # Manage reward populations
‚îÇ   ‚îî‚îÄ‚îÄ evolution_engine.py       # Core evolution logic
‚îú‚îÄ‚îÄ llm_interface/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ local_llm.py             # OLLAMA integration
‚îÇ   ‚îú‚îÄ‚îÄ prompt_templates.py      # Evolution prompts
‚îÇ   ‚îî‚îÄ‚îÄ code_executor.py         # Safe reward function execution
‚îú‚îÄ‚îÄ evaluation/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ performance_tracker.py   # Track RL performance
‚îÇ   ‚îú‚îÄ‚îÄ diversity_metrics.py     # Measure reward diversity
‚îÇ   ‚îî‚îÄ‚îÄ safety_validator.py      # Validate reward functions
‚îú‚îÄ‚îÄ game_logic.py                # Eureka-specific game logic
‚îú‚îÄ‚îÄ game_manager.py              # Eureka experiment management
‚îî‚îÄ‚îÄ main.py                      # CLI interface
```

## üîß **Core Implementation Components**

### **Reward Function Generator**
```python
class RewardFunctionGenerator:
    """
    LLM-based reward function generation and evolution
    
    Design Pattern: Strategy Pattern
    - Different generation strategies (mutation, crossover, novel creation)
    - Pluggable LLM backends for different capabilities
    - Safe code execution environment
    
    Educational Value:
    Demonstrates how LLMs can be used for code generation
    and automated engineering of complex systems.
    """
    
    def __init__(self, llm_interface, safety_validator):
        self.llm = llm_interface
        self.validator = safety_validator
        self.generation_history = []
    
    def generate_initial_population(self, population_size: int = 10):
        """Generate diverse initial reward functions"""
        population = []
        
        base_prompt = """
        Generate a reward function for a Snake game AI agent using reinforcement learning.
        The function should:
        1. Encourage collecting apples
        2. Discourage collisions
        3. Promote efficient movement
        4. Be creative and non-obvious
        
        Return only Python code for the reward function.
        """
        
        for i in range(population_size):
            # Add diversity through prompt variations
            variant_prompt = self._add_diversity_hints(base_prompt, i)
            
            # Generate reward function code
            generated_code = self.llm.generate(variant_prompt)
            
            # Validate and sanitize
            if self.validator.is_safe(generated_code):
                reward_func = self._compile_reward_function(generated_code)
                population.append({
                    'function': reward_func,
                    'code': generated_code,
                    'generation': 0,
                    'parent_ids': [],
                    'performance': None
                })
        
        return population
    
    def evolve_reward_function(self, parent_functions, performance_data):
        """Evolve reward functions based on performance"""
        
        # Select best performers
        top_performers = self._select_top_performers(parent_functions, performance_data)
        
        evolution_prompt = f"""
        Here are the top-performing reward functions and their results:
        
        {self._format_performance_data(top_performers)}
        
        Generate a new, improved reward function that:
        1. Builds upon the successful patterns above
        2. Addresses weaknesses in current approaches
        3. Introduces novel reward mechanisms
        4. Maintains or improves performance
        
        Be creative but maintain code safety and efficiency.
        """
        
        # Generate evolved reward function
        evolved_code = self.llm.generate(evolution_prompt)
        
        if self.validator.is_safe(evolved_code):
            return {
                'function': self._compile_reward_function(evolved_code),
                'code': evolved_code,
                'generation': max([p['generation'] for p in top_performers]) + 1,
                'parent_ids': [p['id'] for p in top_performers],
                'performance': None
            }
        
        return None
```

### **Performance Evaluation System**
```python
class RewardFunctionEvaluator:
    """
    Evaluate reward functions through RL training and game performance
    
    Design Pattern: Template Method Pattern
    - Standard evaluation workflow
    - Customizable metrics and training parameters
    - Comprehensive performance tracking
    """
    
    def __init__(self, rl_algorithm="DQN", training_episodes=1000):
        self.rl_algorithm = rl_algorithm
        self.training_episodes = training_episodes
        self.evaluation_history = []
    
    def evaluate_reward_function(self, reward_function, test_games=100):
        """Comprehensive evaluation of a reward function"""
        
        # 1. Train RL agent with this reward function
        agent = self._create_rl_agent(reward_function)
        training_metrics = self._train_agent(agent, self.training_episodes)
        
        # 2. Test performance on actual games
        game_performance = self._test_game_performance(agent, test_games)
        
        # 3. Analyze reward function properties
        reward_analysis = self._analyze_reward_properties(reward_function)
        
        # 4. Combine metrics
        overall_score = self._calculate_overall_score(
            training_metrics, game_performance, reward_analysis
        )
        
        evaluation_result = {
            'overall_score': overall_score,
            'training_metrics': training_metrics,
            'game_performance': game_performance,
            'reward_analysis': reward_analysis,
            'timestamp': datetime.now()
        }
        
        self.evaluation_history.append(evaluation_result)
        return evaluation_result
    
    def _calculate_overall_score(self, training_metrics, game_performance, reward_analysis):
        """Calculate weighted overall performance score"""
        score = 0
        
        # Game performance (40% weight)
        score += 0.4 * game_performance['average_score'] / 100
        
        # Training efficiency (20% weight)
        score += 0.2 * (1.0 - training_metrics['convergence_time'] / self.training_episodes)
        
        # Stability (20% weight)
        score += 0.2 * (1.0 - game_performance['score_variance'] / game_performance['average_score'])
        
        # Reward function elegance (20% weight)
        score += 0.2 * reward_analysis['elegance_score']
        
        return min(score, 1.0)  # Cap at 1.0
```

### **Evolution Engine**
```python
class EurekaEvolutionEngine:
    """
    Core engine for evolving reward functions using Eureka methodology
    
    Design Pattern: Observer Pattern
    - Notify observers of evolution progress
    - Track performance improvements over generations
    - Enable real-time monitoring and analysis
    """
    
    def __init__(self, generator, evaluator, population_size=20):
        self.generator = generator
        self.evaluator = evaluator
        self.population_size = population_size
        self.generation = 0
        self.best_performance_history = []
        self.observers = []
    
    def evolve(self, max_generations=50, convergence_threshold=0.95):
        """Run the complete evolution process"""
        
        # Initialize population
        self.population = self.generator.generate_initial_population(self.population_size)
        
        for generation in range(max_generations):
            self.generation = generation
            
            # Evaluate all reward functions in parallel
            for individual in self.population:
                if individual['performance'] is None:
                    individual['performance'] = self.evaluator.evaluate_reward_function(
                        individual['function']
                    )
            
            # Track best performance
            best_individual = max(self.population, key=lambda x: x['performance']['overall_score'])
            self.best_performance_history.append(best_individual['performance']['overall_score'])
            
            # Notify observers
            self._notify_observers({
                'generation': generation,
                'best_score': best_individual['performance']['overall_score'],
                'population': self.population
            })
            
            # Check convergence
            if best_individual['performance']['overall_score'] >= convergence_threshold:
                print(f"Converged at generation {generation} with score {best_individual['performance']['overall_score']:.3f}")
                break
            
            # Evolve population
            self._evolve_population()
        
        return self._get_best_reward_function()
    
    def _evolve_population(self):
        """Evolve the population for the next generation"""
        
        # Keep top performers (elitism)
        sorted_population = sorted(self.population, 
                                 key=lambda x: x['performance']['overall_score'], 
                                 reverse=True)
        
        elite_count = self.population_size // 4
        new_population = sorted_population[:elite_count]
        
        # Generate new individuals
        while len(new_population) < self.population_size:
            # Evolve from top performers
            evolved_individual = self.generator.evolve_reward_function(
                sorted_population[:elite_count],
                [ind['performance'] for ind in sorted_population[:elite_count]]
            )
            
            if evolved_individual:
                new_population.append(evolved_individual)
        
        self.population = new_population
```

## üöÄ **Advanced Eureka Features**

### **Multi-Objective Reward Evolution**
- **Performance vs. Elegance**: Balance game performance with code simplicity
- **Stability vs. Exploration**: Balance consistent performance with discovery
- **Efficiency vs. Effectiveness**: Optimize both training speed and final performance
- **Safety vs. Innovation**: Ensure safe reward functions while encouraging creativity

### **Human-AI Collaborative Evolution**
- **Human Feedback Integration**: Incorporate human preferences into evolution
- **Expert Knowledge Injection**: Seed evolution with domain expert insights
- **Interactive Evolution**: Allow human guidance during evolution process
- **Explainable Rewards**: Generate human-interpretable reward functions

### **Meta-Learning Capabilities**
- **Cross-Game Transfer**: Apply learned reward patterns to other games
- **Few-Shot Adaptation**: Quickly adapt to new game variants
- **Curriculum Generation**: Automatically create training curricula
- **Strategy Discovery**: Find novel game-playing strategies

## üéì **Educational and Research Applications**

### **Research Questions**
- Can LLMs discover reward functions better than human experts?
- How does reward function complexity affect training efficiency?
- What reward patterns transfer across different game domains?
- How can we ensure reward function safety and interpretability?

### **Comparative Studies**
- **Eureka vs. Human Design**: Compare evolved vs. hand-crafted rewards
- **Different LLM Backends**: Test various language models for reward generation
- **Evolution Strategies**: Compare different evolutionary approaches
- **Training Algorithms**: Test evolved rewards with different RL algorithms

## üìä **Configuration and Usage**

### **Running Eureka Evolution**
```bash
# Start Eureka evolution with default settings
python main.py --mode evolve --population-size 20 --max-generations 50

# Use specific LLM backend
python main.py --mode evolve --llm-backend ollama --model llama2:13b

# Evaluate specific reward function
python main.py --mode evaluate --reward-function ./evolved_rewards/best_gen_15.py

# Run comparative study
python main.py --mode compare --baseline human_designed --evolved best_evolved
```

### **Configuration Options**
```python
EUREKA_CONFIG = {
    'evolution': {
        'population_size': 20,
        'max_generations': 50,
        'elite_ratio': 0.25,
        'convergence_threshold': 0.95
    },
    'evaluation': {
        'training_episodes': 1000,
        'test_games': 100,
        'parallel_evaluations': 4
    },
    'llm': {
        'backend': 'ollama',
        'model': 'llama2:13b',
        'temperature': 0.7,
        'max_tokens': 2048
    }
}
```

## üîÆ **Future Research Directions**

### **Advanced Evolution Techniques**
- **Differential Evolution**: More sophisticated evolutionary operators
- **Genetic Programming**: Evolve reward function structure, not just parameters
- **Neuroevolution**: Evolve neural network-based reward functions
- **Multi-Population Evolution**: Maintain diverse populations for different objectives

### **Integration with Other Extensions**
- **Heuristics-Informed Evolution**: Use heuristic insights to guide evolution
- **Supervised Learning Integration**: Use ML models to predict reward function performance
- **Human Player Data**: Incorporate human gameplay data into reward design
- **Cross-Domain Transfer**: Apply evolved rewards to other game domains

### **Safety and Interpretability**
- **Formal Verification**: Prove safety properties of evolved reward functions
- **Interpretable Rewards**: Generate human-understandable reward explanations
- **Robustness Testing**: Ensure evolved rewards work across different scenarios
- **Bias Detection**: Identify and mitigate biases in evolved reward functions

---

**Eureka represents a revolutionary approach to reward function design, enabling the discovery of novel and effective strategies through automated LLM-based evolution. This approach can uncover reward patterns that human designers might never consider, leading to breakthrough performance in Snake Game AI while advancing our understanding of automated system design.**

