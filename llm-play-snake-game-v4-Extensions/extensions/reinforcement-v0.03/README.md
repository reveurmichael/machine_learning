# Reinforcement Learning v0.03 â€“ Dashboards, Replay & Web

> Evolution of v0.02; now with user-friendly visualisation and analysis tools.

---

## ğŸŒŸ Key additions

1. **Streamlit dashboard** â€“ real-time loss / reward curves, hyper-parameter sliders.  
2. **Replay systems** â€“ PyGame desktop and Flask web-based viewer.  
3. **Dataset export** â€“ save trajectories to CSV / NPZ for downstream research.  
4. **Modular UI** â€“ dashboard code lives under `dashboard/` to keep `app.py` thin.

Everything else (agents, environment, manager) is **identical to v0.02** to
ensure apples-to-apples comparisons.

---

## ğŸ“ Folder layout

```
extensions/reinforcement-v0.03/
â”œâ”€â”€ __init__.py          # re-exports RLConfig + factory from v0.02
â”œâ”€â”€ README.md            # â† you are here
â”œâ”€â”€ agents/              # âš ï¸ copied verbatim from v0.02 â€“ DO NOT EDIT
â”‚   â””â”€â”€ â€¦
â”œâ”€â”€ game_data.py         # import from v0.02
â”œâ”€â”€ game_logic.py        # import from v0.02
â”œâ”€â”€ game_manager.py      # Thin wrapper adding dashboard hooks
â”œâ”€â”€ dashboard/
â”‚   â”œâ”€â”€ __init__.py      # registers Streamlit tabs
â”‚   â””â”€â”€ training_dashboard.py
â”œâ”€â”€ app.py               # Streamlit entry point
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train.py         # CLI wrapper â€“ identical flags to v0.02
â”‚   â”œâ”€â”€ replay_pygame.py # Desktop replay
â”‚   â””â”€â”€ replay_web.py    # Flask replay
â””â”€â”€ datasets/
    â””â”€â”€ (auto-generated CSV/NPZ saved here)
```

---

## ğŸš€ Quick start (dashboard)

```bash
streamlit run extensions/reinforcement-v0.03/app.py --server.headless true
```
Choose algorithm & parameters in the sidebar, click **Train** â€“ plots update
on the fly.  Replays appear under the **Replay** tab once a run finishes.

---

## ğŸ› ï¸ Under the hood

* **Observer pattern** â€“ `RLGameManager` pushes episode stats to the
  `dashboard.training_dashboard.MetricsBuffer` which emits Streamlit updates.
* **Factory pattern** â€“ unchanged agent registry from v0.02.
* **Template-Method** â€“ manager hooks call base training loop, injecting UI.
* **Singleton** â€“ shared FileManager guarantees only one active log path.

---

## ğŸ“ˆ Upgrade path

| Version | Focus |
|---------|-------|
| v0.01 | Proof-of-concept DQN |
| v0.02 | Multi-algorithm & headless training |
| **v0.03** | Visualisation, replay, dataset export â† **YOU ARE HERE** |
| v0.04 | *Not planned* â€“ RL stabilised |

---

*Last updated 2025-06-23* 