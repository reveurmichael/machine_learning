# Reinforcement Learning v0.02 â€“ Multi-Algorithm Expansion

> Second-citizen extension; works stand-alone together with `extensions/common/`.  
> Builds on v0.01 (DQN-only) and introduces **algorithm diversity**.

---

## ğŸš€ What's new compared to v0.01?

| Area | v0.01 | **v0.02** |
|------|-------|-----------|
| Algorithms | DQN | **DQN, PPO, A3C, SAC** |
| Agents folder | âŒ single file | **âœ… `agents/` package** with one module per algorithm |
| CLI | `train.py` (DQN-centric) | **Unified CLI** (`scripts/train.py`) with `--algorithm` switch |
| Game logic | Heavy environment | **Lightweight gym-style env** (`game_logic.py`) â€“ faster prototyping |
| Observability | Basic prints | Observer hooks for episode complete events |
| Docs | Minimal | **This README** ğŸ˜„ |

No GUI / replay yet â€“ those arrive in v0.03, in line with the global roadmap.

---

## ğŸ—ï¸ Folder layout

```
extensions/reinforcement-v0.02/
â”œâ”€â”€ __init__.py          # RLConfig + agent factory
â”œâ”€â”€ README.md            # â† you are here
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ __init__.py      # Protocol + helper
â”‚   â”œâ”€â”€ agent_dqn.py     # Functional mini-DQN
â”‚   â”œâ”€â”€ agent_ppo.py     # PPO stub
â”‚   â”œâ”€â”€ agent_a3c.py     # A3C stub
â”‚   â””â”€â”€ agent_sac.py     # SAC stub
â”œâ”€â”€ game_data.py         # RLGameData container
â”œâ”€â”€ game_logic.py        # Lightweight env
â”œâ”€â”€ game_manager.py      # Training loop & logging
â””â”€â”€ scripts/
    â””â”€â”€ train.py         # CLI â€“ headless training
```

---

## ğŸ”§ Quick start

1. **Install deps** (only NumPy required for stubs):
   ```bash
   pip install numpy
   ```

2. **Train DQN** for 1 000 episodes on 10Ã—10 grid:
   ```bash
   python extensions/reinforcement-v0.02/scripts/train.py \
       --algorithm DQN --episodes 1000
   ```

3. **Switch algorithm** (e.g. PPO):
   ```bash
   python extensions/reinforcement-v0.02/scripts/train.py \
       --algorithm PPO --episodes 500
   ```

Models are saved under:
```
logs/extensions/models/grid-size-N/reinforcement-<algo>_{timestamp}/
```
Training metrics print to stdout; connect your own observer to stream to
TensorBoard.

---

## ğŸ¨ Design patterns in play

1. **Factory Pattern** â€“ `create_rl_agent()` selects the proper class from the
   registry; adding a new algorithm is a two-line diff.
2. **Template-Method** â€“ `RLGameManager.run()` drives the training while agent
   strategy varies.
3. **Observer** â€“ agents emit `episode_complete` which the manager logs; can be
   extended for dashboards.
4. **Singleton** â€“ underlying FileManager (shared via `extensions/common`) keeps
   logs coerently organised.

All classes are documented for educational clarity.

---

## ğŸ“ˆ Roadmap

| Version | Focus |
|---------|-------|
| v0.03 | Streamlit dashboard, PyGame & web replay, gym-compatible wrappers |
---

