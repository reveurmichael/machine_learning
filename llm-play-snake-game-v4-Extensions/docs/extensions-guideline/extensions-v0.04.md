## üß© General Overview

**v0.04** focuses exclusively on **dataset generation with natural language explanations**, specifically for the **heuristics** family of Snake game agents.

The core feature of v0.04 is the ability to generate **CSV datasets** that include both:

* **Structured decision data** (e.g., board state, direction, position),
* and **rich natural language descriptions** explaining **why each decision is made**.

This dataset is intended for **fine-tuning large language models (LLMs)** in Task 4, enabling them to learn and imitate heuristic algorithms through language-grounded reasoning.

---

## ü§ñ Heuristics v0.04

v0.04 applies **only to heuristics**.
There is **no v0.04** planned for:

* Supervised learning models
* Genetic or evolutionary algorithms
* reinforcement learning models

### üîÅ Built on Top of v0.03

Since v0.04 extends v0.03, it inherits the web interface, replay system, and dataset export infrastructure. However, v0.04 introduces a crucial innovation:

> At **each key decision point** in the heuristic logic, the agent will **generate detailed natural language explanations** describing the reasoning behind its action.

These explanations will be logged alongside standard metadata (e.g., grid state, next move) in CSV format.

### üß† LLM Fine-Tuning Use Case (Task 4)

These **language-rich CSV datasets** are designed specifically to:

* Teach LLMs how to play the Snake game
* Ground decision-making in explicit, interpretable reasoning
* Provide a training corpus for supervised fine-tuning (e.g., SFT or RFT) of open-weight models

Example data entries will include columns like:

| step | head\_pos | apple\_pos | move  | reasoning\_sentence                                                                    |
| ---- | --------- | ---------- | ----- | -------------------------------------------------------------------------------------- |
| 14   | (3,4)     | (6,7)      | RIGHT | ‚ÄúThe apple is to the upper-right, and RIGHT leads me closer while avoiding obstacles.‚Äù |

This forms the bridge from classic symbolic reasoning to neural policy learning.

---


## Maybe not CSV, but JSON JSONL ?

Great follow-up! For **fine-tuning large language models (LLMs)**, the dataset format and structure are quite different compared to traditional supervised learning models. Here's what you should consider:

---

### Dataset Format for Fine-Tuning LLMs

**1. Plain Text / JSONL (JSON Lines) ‚Äî Most Common**

* **Format:** Usually, fine-tuning datasets for LLMs are stored in plain text or JSONL files.
* **Why?**

  * LLMs are trained on text tokens, so data needs to be text-based (natural language).
  * JSONL allows one JSON object per line, making it easy to store structured text pairs like prompt-completion, or input-output examples.
* **Example:**

```json
{"prompt": "Here is the current state of the game, head blabla, apple blabla, snake blabla, etc", "completion": "Move RIGHT by one cell to (x+1, y), because blabla (those texts are generated by the heuristics)"}
```

---

**2. Structured JSON or YAML**

* Sometimes more structured data is needed, like when including metadata, reasoning chains, or annotations.
* Useful if your heuristics produce **long language explanations or step-by-step reasoning** at each decision point.

---

**3. Text with Special Tokens**

* When fine-tuning, special tokens may be introduced to delineate prompts, instructions, or to mark start/end of completions.
* This is especially relevant for instruction-tuning or RLHF-style datasets.

---

### Why Not CSV/NPZ/Parquet for LLM Fine-Tuning?

* LLMs expect **sequences of tokens**, not tabular or numerical feature vectors.
* While technically possible to convert structured data to string format, CSV and binary formats are **not standard or efficient** for LLM fine-tuning pipelines.
* The dataset must be easily consumable by tokenizers and data loaders specific to language models.

---

### Dataset Structure & Content Considerations

Since you mentioned heuristics v0.04 will include **long language sentences explaining heuristic decisions**, your dataset might look like:

* **At each decision point (step):**

  * The current state description (in language)
  * The heuristic explanation (why a particular move was chosen)
  * The resulting action
  * Possibly a reward or outcome summary

This can be structured as a sequence of examples, each with prompt + completion text.

---

### Example Dataset Entry for Heuristic Explanation Fine-Tuning

```jsonl
{"prompt": "State: Snake head at (4,4), apple at (6,7). What move do you make?", 
 "completion": "I choose to move RIGHT because it shortens the distance to the apple while avoiding collisions."}
```

Or for longer explanations:

```jsonl
{"prompt": "Decision point 15:\nCurrent snake state and environment details...\nWhat is the reasoning and action?",
 "completion": "The heuristic evaluates that moving UP is safest because it leads to the open space, avoiding the snake's tail and obstacles. Therefore, the snake moves UP."}
```

## JSON VS JSONL

Great question! Here's a clear comparison between **JSON** and **JSONL (JSON Lines)** formats:

---

### JSON (JavaScript Object Notation)

* **Structure:** A single JSON document, often an array of objects.
* **Example:**

```json
[
  {"prompt": "Move right", "completion": "Snake moves right"},
  {"prompt": "Move up", "completion": "Snake moves up"}
]
```

* **Characteristics:**

  * The entire dataset is wrapped in a single JSON array or object.
  * Must be fully loaded and parsed at once by the program.
  * Can become very large and memory-intensive for big datasets.
  * Less suitable for streaming or incremental reading.

---

### JSONL (JSON Lines)

* **Structure:** Each line is a separate JSON object.
* **Example:**

```json
{"prompt": "Move right", "completion": "Snake moves right"}
{"prompt": "Move up", "completion": "Snake moves up"}
```

* **Characteristics:**

  * Easy to process line-by-line or stream.
  * More scalable for very large datasets.
  * Allows appending new data easily by adding new lines.
  * Supported by many ML frameworks (e.g., Hugging Face datasets expect JSONL).
  * Simpler to handle in shell commands and Unix pipelines.

---

### Which to Choose for LLM Fine-Tuning?

* **JSONL is preferred** because:

  * Allows incremental loading without memory overhead.
  * Fits better with data loading pipelines for tokenizers and training frameworks.
  * Easier to append, update, or split datasets.
  * Standard format in the LLM fine-tuning community.
  
---

### Summary for LLM Fine-Tuning Dataset Format:

| Format          | Typical Usage                   | Notes                                       |
| --------------- | ------------------------------- | ------------------------------------------- |
| JSONL (text)    | Prompt-completion pairs         | Standard for LLM fine-tuning                |
| Plain Text      | Raw or formatted text sequences | Simple but less structured                  |
| JSON/YAML       | Complex data with annotations   | Useful for metadata or multi-field examples |
| CSV/NPZ/Parquet | Not typical / Not recommended   | Not aligned with token-based input          |

---

### Final Advice:

* Design your dataset generation for heuristics v0.04 to **output JSONL files** with rich natural language explanations.
* Include clear prompt and completion fields for each decision step.
* Consider tokenization and model input length constraints (split or chunk long explanations if needed).








## üì¶ Supervised Learning Models

There is **no v0.04 for supervised learning models**.

The existing supervised extensions (v0.01‚Äìv0.03) already:

* Train on datasets exported from heuristics v0.03
* Cover a wide variety of models (XGBoost, LightGBM, PyTorch NN/CNN/RNN/GNN)
* Include training, evaluation, and visualization components

These models do **not require** language-annotated data at this time.

---

## ‚úÖ Summary

| Category           | v0.04 Support | Notes                                                               |
| ------------------ | ------------- | ------------------------------------------------------------------- |
| Heuristics         | ‚úÖ Yes         | CSV + language reasoning for LLM fine-tuning                        |
| Supervised Models  | ‚ùå No          | v0.03 is sufficient for training + evaluation                       |
| Genetic Algorithms | ‚ùå No          | v0.03 remains the latest; not involved in language generation tasks |

**v0.04 = Dataset for LLM fine-tuning with rich explanations.**
