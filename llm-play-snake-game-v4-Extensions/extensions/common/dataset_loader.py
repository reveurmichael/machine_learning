"""
Dataset Loading Utilities for Snake Game Extensions
==================================================

Provides utilities for loading and processing datasets generated by heuristics
extensions. Works with the common CSV schema to ensure compatibility across
different grid sizes and model types.

Design Pattern: Strategy Pattern
- Different data loading strategies for different formats
- Pluggable preprocessing for different model types
- Consistent interface across all extensions
"""

from __future__ import annotations

import os
import pandas as pd
import numpy as np
from pathlib import Path
from typing import Dict, List, Any, Tuple, Optional, Union
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder

from .csv_schema import get_schema_info, validate_game_state
from .path_utils import setup_extension_paths
setup_extension_paths()


class DatasetLoader:
    """
    Loads and preprocesses datasets for supervised learning models.
    
    Handles different grid sizes and data formats, ensuring compatibility
    with the common CSV schema.
    """
    
    def __init__(self, grid_size: int = 10):
        """
        Initialize dataset loader.
        
        Args:
            grid_size: Size of the game grid
        """
        self.grid_size = grid_size
        self.schema_info = get_schema_info(grid_size)
        self.feature_names = self.schema_info["feature_names"]
        self.target_column = self.schema_info["target_column"]
        
        # Preprocessing components
        self.scaler = StandardScaler()
        self.label_encoder = LabelEncoder()
        self.is_fitted = False
    
    def load_csv_dataset(self, filepath: str) -> pd.DataFrame:
        """
        Load a CSV dataset file.
        
        Args:
            filepath: Path to the CSV file
            
        Returns:
            Loaded DataFrame
            
        Raises:
            ValueError: If the dataset doesn't match the expected schema
        """
        if not os.path.exists(filepath):
            raise FileNotFoundError(f"Dataset file not found: {filepath}")
        
        # Load the dataset
        df = pd.read_csv(filepath)
        
        # Validate the dataset structure
        self._validate_dataset_structure(df)
        
        return df
    
    def load_multiple_datasets(self, filepaths: List[str]) -> pd.DataFrame:
        """
        Load and combine multiple CSV datasets.
        
        Args:
            filepaths: List of paths to CSV files
            
        Returns:
            Combined DataFrame
        """
        if not filepaths:
            raise ValueError("No file paths provided")
        
        dataframes = []
        for filepath in filepaths:
            try:
                df = self.load_csv_dataset(filepath)
                dataframes.append(df)
                print(f"Loaded dataset: {filepath} ({len(df)} samples)")
            except Exception as e:
                print(f"Warning: Could not load {filepath}: {e}")
                continue
        
        if not dataframes:
            raise ValueError("No valid datasets could be loaded")
        
        # Combine all dataframes
        combined_df = pd.concat(dataframes, ignore_index=True)
        print(f"Combined dataset: {len(combined_df)} total samples")
        
        return combined_df
    
    def load_from_directory(self, directory_path: str, pattern: str = "*.csv") -> pd.DataFrame:
        """
        Load all CSV files from a directory.
        
        Args:
            directory_path: Path to directory containing CSV files
            pattern: File pattern to match (default: "*.csv")
            
        Returns:
            Combined DataFrame from all matching files
        """
        directory = Path(directory_path)
        if not directory.exists():
            raise FileNotFoundError(f"Directory not found: {directory_path}")
        
        # Find all CSV files
        csv_files = list(directory.glob(pattern))
        if not csv_files:
            raise FileNotFoundError(f"No CSV files found in {directory_path}")
        
        # Load all datasets
        filepaths = [str(f) for f in csv_files]
        return self.load_multiple_datasets(filepaths)
    
    def _validate_dataset_structure(self, df: pd.DataFrame) -> None:
        """
        Validate that the dataset has the expected structure.
        
        Args:
            df: DataFrame to validate
            
        Raises:
            ValueError: If the dataset structure is invalid
        """
        # Check required columns
        required_columns = self.schema_info["column_names"]
        missing_columns = set(required_columns) - set(df.columns)
        
        if missing_columns:
            raise ValueError(f"Missing required columns: {missing_columns}")
        
        # Check target column values
        valid_moves = ["UP", "DOWN", "LEFT", "RIGHT"]
        invalid_moves = set(df[self.target_column].unique()) - set(valid_moves)
        
        if invalid_moves:
            raise ValueError(f"Invalid target moves found: {invalid_moves}")
        
        # Check data types for numeric features
        for feature in self.feature_names:
            if feature in df.columns:
                if not pd.api.types.is_numeric_dtype(df[feature]):
                    raise ValueError(f"Feature {feature} is not numeric")
    
    def prepare_features_and_targets(self, df: pd.DataFrame, 
                                   scale_features: bool = True) -> Tuple[np.ndarray, np.ndarray]:
        """
        Prepare features and targets for training.
        
        Args:
            df: Input DataFrame
            scale_features: Whether to scale features using StandardScaler
            
        Returns:
            Tuple of (features, targets) as numpy arrays
        """
        # Extract features
        X = df[self.feature_names].values.astype(np.float32)
        
        # Extract and encode targets
        y = df[self.target_column].values
        
        # Scale features if requested
        if scale_features:
            if not self.is_fitted:
                X = self.scaler.fit_transform(X)
                self.is_fitted = True
            else:
                X = self.scaler.transform(X)
        
        # Encode targets
        y_encoded = self.label_encoder.fit_transform(y)
        
        return X, y_encoded
    
    def split_dataset(self, X: np.ndarray, y: np.ndarray, 
                     test_size: float = 0.2, val_size: float = 0.2,
                     random_state: int = 42) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
        """
        Split dataset into train, validation, and test sets.
        
        Args:
            X: Feature array
            y: Target array
            test_size: Proportion for test set
            val_size: Proportion for validation set (from remaining data)
            random_state: Random seed for reproducibility
            
        Returns:
            Tuple of (X_train, X_val, X_test, y_train, y_val, y_test)
        """
        # First split: train+val vs test
        X_temp, X_test, y_temp, y_test = train_test_split(
            X, y, test_size=test_size, random_state=random_state, stratify=y
        )
        
        # Second split: train vs val
        val_size_adjusted = val_size / (1 - test_size)
        X_train, X_val, y_train, y_val = train_test_split(
            X_temp, y_temp, test_size=val_size_adjusted, random_state=random_state, stratify=y_temp
        )
        
        return X_train, X_val, X_test, y_train, y_val, y_test
    
    def get_dataset_info(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        Get information about the dataset.
        
        Args:
            df: Input DataFrame
            
        Returns:
            Dictionary with dataset information
        """
        # Basic statistics
        info = {
            "total_samples": len(df),
            "feature_count": len(self.feature_names),
            "grid_size": self.grid_size,
            "schema_info": self.schema_info,
        }
        
        # Target distribution
        target_counts = df[self.target_column].value_counts()
        info["target_distribution"] = target_counts.to_dict()
        
        # Feature statistics
        feature_stats = {}
        for feature in self.feature_names:
            if feature in df.columns:
                feature_stats[feature] = {
                    "mean": df[feature].mean(),
                    "std": df[feature].std(),
                    "min": df[feature].min(),
                    "max": df[feature].max(),
                }
        info["feature_statistics"] = feature_stats
        
        # Game statistics
        if "game_id" in df.columns:
            info["unique_games"] = df["game_id"].nunique()
            info["avg_steps_per_game"] = df.groupby("game_id").size().mean()
        
        return info
    
    def save_preprocessing_state(self, filepath: str) -> None:
        """
        Save preprocessing state (scaler, label encoder) for later use.
        
        Args:
            filepath: Path to save the preprocessing state
        """
        import pickle
        
        state = {
            "scaler": self.scaler,
            "label_encoder": self.label_encoder,
            "grid_size": self.grid_size,
            "feature_names": self.feature_names,
            "is_fitted": self.is_fitted
        }
        
        with open(filepath, 'wb') as f:
            pickle.dump(state, f)
    
    def load_preprocessing_state(self, filepath: str) -> None:
        """
        Load preprocessing state from file.
        
        Args:
            filepath: Path to the preprocessing state file
        """
        import pickle
        
        with open(filepath, 'rb') as f:
            state = pickle.load(f)
        
        self.scaler = state["scaler"]
        self.label_encoder = state["label_encoder"]
        self.grid_size = state["grid_size"]
        self.feature_names = state["feature_names"]
        self.is_fitted = state["is_fitted"]


def detect_grid_size_from_dataset(filepath: str) -> int:
    """
    Detect the grid size from a dataset file.
    
    Args:
        filepath: Path to the dataset file
        
    Returns:
        Detected grid size
        
    Raises:
        ValueError: If grid size cannot be detected
    """
    try:
        df = pd.read_csv(filepath)
        
        # Try to detect from column names or data
        if "head_x" in df.columns and "head_y" in df.columns:
            max_x = df["head_x"].max()
            max_y = df["head_y"].max()
            max_coord = max(max_x, max_y)
            
            # Common grid sizes
            common_sizes = [8, 10, 12, 16, 20]
            for size in common_sizes:
                if max_coord < size:
                    return size
            
            # If not found in common sizes, return the next power of 2
            return 2 ** (max_coord.bit_length())
        
        raise ValueError("Cannot detect grid size from dataset structure")
        
    except Exception as e:
        raise ValueError(f"Error detecting grid size: {e}")


def load_dataset_for_training(dataset_paths: Union[str, List[str]], 
                            grid_size: Optional[int] = None,
                            scale_features: bool = True,
                            test_size: float = 0.2,
                            val_size: float = 0.2) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, Dict[str, Any]]:
    """
    Convenience function to load and prepare a dataset for training.
    
    Args:
        dataset_paths: Path(s) to dataset file(s) or directory
        grid_size: Grid size (auto-detected if None)
        scale_features: Whether to scale features
        test_size: Proportion for test set
        val_size: Proportion for validation set
        
    Returns:
        Tuple of (X_train, X_val, X_test, y_train, y_val, y_test, dataset_info)
    """
    # Auto-detect grid size if not provided
    if grid_size is None:
        if isinstance(dataset_paths, str):
            grid_size = detect_grid_size_from_dataset(dataset_paths)
        else:
            grid_size = detect_grid_size_from_dataset(dataset_paths[0])
        print(f"Auto-detected grid size: {grid_size}")
    
    # Create loader
    loader = DatasetLoader(grid_size)
    
    # Load dataset
    if isinstance(dataset_paths, str):
        if os.path.isdir(dataset_paths):
            df = loader.load_from_directory(dataset_paths)
        else:
            df = loader.load_csv_dataset(dataset_paths)
    else:
        df = loader.load_multiple_datasets(dataset_paths)
    
    # Get dataset info
    dataset_info = loader.get_dataset_info(df)
    print(f"Dataset info: {dataset_info['total_samples']} samples, {dataset_info['feature_count']} features")
    
    # Prepare features and targets
    X, y = loader.prepare_features_and_targets(df, scale_features=scale_features)
    
    # Split dataset
    X_train, X_val, X_test, y_train, y_val, y_test = loader.split_dataset(
        X, y, test_size=test_size, val_size=val_size
    )
    
    print(f"Dataset split: Train={len(X_train)}, Val={len(X_val)}, Test={len(X_test)}")
    
    return X_train, X_val, X_test, y_train, y_val, y_test, dataset_info


# Example usage
if __name__ == "__main__":
    # Example: Load dataset from heuristics v0.03 output
    try:
        # This would be the path to datasets generated by heuristics v0.03
        dataset_path = "../../logs/extensions/datasets/grid-size-10/tabular_bfs_data.csv"
        
        X_train, X_val, X_test, y_train, y_val, y_test, info = load_dataset_for_training(
            dataset_path, grid_size=10
        )
        
        print("Dataset loaded successfully!")
        print(f"Feature shape: {X_train.shape}")
        print(f"Target shape: {y_train.shape}")
        
    except Exception as e:
        print(f"Error loading dataset: {e}")
        print("This is expected if the dataset file doesn't exist yet.") 