# Reinforcement Learning Extensions

> **Important â€” Authoritative Reference:** This document serves as a **GOOD_RULES** authoritative reference for reinforcement learning extension standards and supplements the _Final Decision Series_ and extension guidelines.

## ðŸŽ¯ **RL Philosophy in Extension Architecture**

Reinforcement learning extensions demonstrate how agents learn optimal policies through interaction with the Snake game environment. They follow the standardized extension evolution patterns while addressing the unique challenges of RL training and evaluation.

### **SUPREME_RULES Alignment**
- **SUPREME_RULE NO.1**: Enforces reading all GOOD_RULES before making RL architectural changes to ensure comprehensive understanding
- **SUPREME_RULE NO.2**: Uses precise `final-decision-N.md` format consistently when referencing architectural decisions and RL patterns
- **SUPREME_RULE NO.3**: Enables lightweight common utilities with OOP extensibility while maintaining RL-specific patterns through inheritance rather than tight coupling

### **Core RL Principles**
- **Learning Through Interaction**: Agents improve through trial and error
- **Delayed Rewards**: Optimize for long-term rather than immediate gains
- **Exploration vs. Exploitation**: Balance discovering new strategies with using known good ones
- **Policy Evolution**: Demonstrate how strategies improve over time

## ðŸ§  **RL Algorithm Portfolio**

### **Value-Based Methods**
- **DQN (Deep Q-Network)**: Foundation of deep RL with experience replay
- **Double DQN**: Addresses overestimation bias in Q-learning
- **Dueling DQN**: Separates state value and action advantage estimation

### **Policy-Based Methods**
- **PPO (Proximal Policy Optimization)**: Stable policy gradient method
- **A3C (Asynchronous Advantage Actor-Critic)**: Distributed training approach
- **SAC (Soft Actor-Critic)**: Maximum entropy RL for robust policies

### **Educational Progression**
Following the extension evolution pattern:
- **v0.01**: Single DQN implementation (proof of concept)
- **v0.02**: Multiple RL algorithms with factory patterns
- **v0.03**: Training dashboard and model persistence

## ðŸ—ï¸ **Architecture Integration**

### **Following GOOD_RULES Patterns**
RL extensions adhere to established architectural decisions:

**Agent Naming (Final Decision 4)**:
```python
agent_dqn.py           â†’ class DQNAgent(BaseAgent)
agent_ppo.py           â†’ class PPOAgent(BaseAgent)
agent_a3c.py           â†’ class A3CAgent(BaseAgent)
```

**Factory Pattern (Final Decision 7-8)**:
```python
class RLAgentFactory:
    """Factory for creating RL agents"""
    
    _agent_registry = {
        "DQN": DQNAgent,
        "PPO": PPOAgent,
        "A3C": A3CAgent,
    }
    
    @classmethod
    def create_agent(cls, algorithm: str, **kwargs) -> BaseAgent:
        """Create RL agent by algorithm name"""
        return cls._agent_registry[algorithm.upper()](**kwargs)
```

**Path Management (Final Decision 6)**:
```python
from extensions.common.path_utils import get_model_path

# RL model storage
model_path = get_model_path(
    extension_type="reinforcement",
    version="0.02",
    grid_size=grid_size,
    algorithm="dqn",
    timestamp=timestamp
)
```

## ðŸŽ“ **RL-Specific Considerations**

### **RL State Representation Strategy**
Reinforcement learning often benefits from raw or minimally processed state representations:

| Representation Type | RL Algorithm | Why It Works |
|-------------------|-------------|--------------|
| **Raw Board State** | **DQN, A3C, PPO** | Direct policy learning from grid |
| **16-Feature Tabular** | Q-Learning (classical) | State space compression |
| **Sequential NPZ** | LSTM-based RL | Temporal pattern recognition |
| **Spatial 2D Arrays** | CNN-based DQN | Spatial pattern extraction |
| **Graph Structures** | Graph-based RL | Relationship-aware policies |

**RL-Specific Advantages:**
- **Raw States**: Enable end-to-end learning without feature engineering
- **Spatial Arrays**: Leverage convolutions for spatial understanding
- **Sequential Data**: Learn from temporal patterns and trajectories
- **Direct Policy Learning**: Avoid hand-crafted feature bias

### **Environment Integration**
RL agents interact with the Snake game through a consistent interface:
```python
class RLGameLogic(BaseGameLogic):
    """RL-specific game logic with environment interface"""
    
    def get_observation(self):
        """Return current state observation for RL agent"""
        return self.game_state_adapter.get_observation()
    
    def apply_action(self, action):
        """Apply RL agent action and return reward, done, info"""
        reward = self.calculate_reward(action)
        done = self.check_terminal_state()
        return reward, done, {}
```

### **Training Infrastructure**
RL training follows the script-runner philosophy:
- **Training Scripts**: CLI-based training in `scripts/` folder
- **Streamlit Dashboard**: UI for launching training with parameter control
- **Model Persistence**: Standardized saving/loading of trained models
- **Progress Monitoring**: Training metrics and visualization

### **Dataset Generation**
RL extensions participate in the multi-directional data ecosystem:
- **Experience Data**: Generate datasets from training experience
- **Policy Trajectories**: Export successful game sequences
- **Learning Curves**: Training progress data for analysis
- **Model Checkpoints**: Intermediate models for comparative studies

## ðŸš€ **Implementation Guidelines**

### **Configuration Management**
Following Final Decision 2:
```python
from extensions.common.config.rl_constants import (
    DEFAULT_LEARNING_RATE,
    DEFAULT_BATCH_SIZE,
    DEFAULT_EXPLORATION_RATE,
    MAX_TRAINING_EPISODES
)
```

### **Base Class Integration**
RL agents extend the standard agent hierarchy:
```python
class RLAgent(BaseAgent):
    """Base class for all RL agents"""
    
    @abstractmethod
    def select_action(self, observation, training=True):
        """Select action given current observation"""
        pass
    
    @abstractmethod
    def update(self, experience):
        """Update agent parameters from experience"""
        pass
    
    @abstractmethod
    def save_model(self, path):
        """Save trained model to path"""
        pass
```

### **Training Loop Integration**
RL training integrates with the base game loop:
```python
class RLTrainingLoop(BaseGameLoop):
    """Training loop for RL agents"""
    
    def _get_new_plan(self):
        """Get action from RL agent and update if training"""
        observation = self.game_logic.get_observation()
        action = self.agent.select_action(observation, training=True)
        
        if self.training_mode:
            experience = self.collect_experience(action)
            self.agent.update(experience)
        
        self.game.planned_moves = [action]
```

## ðŸ”® **Future Directions**

### **Advanced RL Techniques**
- **Multi-Agent RL**: Competitive and cooperative Snake scenarios
- **Hierarchical RL**: Learning high-level strategies and low-level tactics
- **Meta-Learning**: Agents that learn to learn new tasks quickly
- **Curriculum Learning**: Progressive difficulty in training scenarios

### **Cross-Extension Integration**
- **Heuristic-Guided RL**: Using pathfinding algorithms to initialize RL policies
- **Supervised Pre-training**: Initialize RL agents with supervised learning
- **Eureka Integration**: Automated reward function discovery for RL
- **Transfer Learning**: Apply RL models across different grid sizes

---

**Reinforcement learning extensions demonstrate the power of learning through interaction while maintaining architectural consistency with the broader Snake Game AI project. By following established patterns while addressing RL-specific challenges, these extensions provide educational insights into modern deep learning approaches to sequential decision making.**
